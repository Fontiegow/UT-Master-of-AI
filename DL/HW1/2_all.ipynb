{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7187c08",
   "metadata": {},
   "source": [
    "# **Question 2 - Perceptron Experiments (Stagewise Implementation)**  \n",
    "This notebook implements Question 2 in a **stagewise manner**, meaning:  \n",
    "- **(a)** Best weight initialization method is found first.  \n",
    "- **(b)** Then, the best threshold (θ) is determined using that initialization.  \n",
    "- **(c)** Finally, the best learning rate (η) is found using the optimal init and θ values.  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3667f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_char(path):\n",
    "    \"\"\"Convert a text file (9x7 character image) into a 1D binary vector.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    data = np.array([[1 if c == '#' else 0 for c in line] for line in lines], dtype=float)\n",
    "    return data.flatten()\n",
    "\n",
    "def load_dataset(folder):\n",
    "    \"\"\"\n",
    "    Load dataset of character text files.\n",
    "    A → label +1\n",
    "    others → label -1\n",
    "    \"\"\"\n",
    "    X, y, names = [], [], []\n",
    "    for fname in sorted(os.listdir(folder)):\n",
    "        if not fname.endswith(\".txt\"):\n",
    "            continue\n",
    "        label = fname[0].upper()\n",
    "        target = 1 if label == 'A' else -1\n",
    "        x = read_char(os.path.join(folder, fname))\n",
    "        X.append(x)\n",
    "        y.append(target)\n",
    "        names.append(fname)\n",
    "    return np.array(X), np.array(y), names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7fa19c",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "These helper functions include:\n",
    "- `init_weights()`: initialize weights with different strategies\n",
    "- `train_one_epoch()`: single-epoch perceptron update rule\n",
    "- `train_perceptron()`: multiple-epoch training\n",
    "- `load_dataset()`: placeholder for your dataset loading function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8fca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(mode, dim, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if mode == \"zeros\":\n",
    "        return np.zeros(dim), 0.0\n",
    "    elif mode == \"uniform\":\n",
    "        return rng.uniform(-0.5, 0.5, dim), float(rng.uniform(-0.5, 0.5))\n",
    "    elif mode == \"normal\":\n",
    "        return rng.normal(0, 0.01, dim), float(rng.normal(0, 0.01))\n",
    "    elif mode == \"xavier\":\n",
    "        std = np.sqrt(2.0 / (dim + 1.0))\n",
    "        return rng.normal(0, std, dim), float(rng.normal(0, std))\n",
    "    elif mode == \"kaiming\":\n",
    "        std = np.sqrt(2.0 / dim)\n",
    "        return rng.normal(0, std, dim), float(rng.normal(0, std))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown initialization mode\")\n",
    "\n",
    "\n",
    "def train_one_epoch(X, y, w, b, lr, theta=0.0):\n",
    "    errors = 0\n",
    "    for xi, target in zip(X, y):\n",
    "        z = np.dot(w, xi) + b\n",
    "        y_pred = 1 if z >= theta else -1\n",
    "        if y_pred != target:\n",
    "            w += lr * target * xi\n",
    "            b += lr * target\n",
    "            errors += 1\n",
    "    preds = np.where(np.dot(X, w.T) + b >= theta, 1, -1)\n",
    "    error_rate = np.mean(preds != y)\n",
    "    return w, b, errors, error_rate\n",
    "\n",
    "\n",
    "def train_perceptron(X, y, w_init, b_init, lr=0.1, theta=0.0, max_epochs=100):\n",
    "    w, b = w_init.copy(), float(b_init)\n",
    "    epoch_updates, epoch_errors = [], []\n",
    "    start_time = time.time()\n",
    "    converged_epoch = None\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        w, b, updates, err_rate = train_one_epoch(X, y, w, b, lr, theta)\n",
    "        epoch_updates.append(updates)\n",
    "        epoch_errors.append(err_rate)\n",
    "        if updates == 0 and converged_epoch is None:\n",
    "            converged_epoch = epoch\n",
    "            break\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    return {\n",
    "        \"w\": w, \"b\": b,\n",
    "        \"epoch_updates\": epoch_updates,\n",
    "        \"epoch_errors\": epoch_errors,\n",
    "        \"converged_epoch\": converged_epoch or max_epochs,\n",
    "        \"elapsed\": elapsed\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920256a",
   "metadata": {},
   "source": [
    "## Stagewise Experiment Function\n",
    "This function runs the experiments in three stages:\n",
    "1. Find the best initialization\n",
    "2. Find the best threshold θ\n",
    "3. Find the best learning rate η\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bcd5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_stagewise(train_folder=\"Characters-TrainSet\",\n",
    "                         test_folder=\"Characters-TestSet\",\n",
    "                         init_modes=[\"zeros\",\"uniform\",\"normal\",\"xavier\",\"kaiming\"],\n",
    "                         theta_list=[-1.0, -0.5, 0.0, 0.5, 1.0],\n",
    "                         eta_list=[0.01, 0.05, 0.1, 0.3],\n",
    "                         max_epochs=30,\n",
    "                         seed_base=123):\n",
    "\n",
    "    X_train, y_train, _ = load_dataset(train_folder)\n",
    "    X_test, y_test, _ = load_dataset(test_folder)\n",
    "    dim = X_train.shape[1]\n",
    "\n",
    "    # -------------------------\n",
    "    # (A) Initialization Stage\n",
    "    # -------------------------\n",
    "    print(\"\\n=== Stage A: Finding best initialization mode ===\")\n",
    "    init_results = []\n",
    "\n",
    "    for i_mode, mode in enumerate(init_modes):\n",
    "        seed = seed_base + i_mode * 100\n",
    "        w0, b0 = init_weights(mode, dim, seed)\n",
    "        result = train_perceptron(X_train, y_train, w0, b0, lr=0.1, theta=0.0, max_epochs=max_epochs)\n",
    "\n",
    "        y_pred_test = np.where(np.dot(X_test, result[\"w\"]) + result[\"b\"] >= 0.0, 1, -1)\n",
    "        test_err = np.mean(y_pred_test != y_test)\n",
    "        init_results.append((mode, test_err, result))\n",
    "\n",
    "        print(f\"{mode:7s} → test error={test_err*100:.2f}% conv={result['converged_epoch']}\")\n",
    "\n",
    "    best_mode, best_err, best_result = min(init_results, key=lambda x: x[1])\n",
    "    print(f\"\\n✅ Best init mode: {best_mode} (error={best_err*100:.2f}%)\")\n",
    "\n",
    "    # -------------------------\n",
    "    # (B) Threshold Stage\n",
    "    # -------------------------\n",
    "    print(\"\\n=== Stage B: Finding best threshold (θ) ===\")\n",
    "    theta_results = []\n",
    "    for theta in theta_list:\n",
    "        w0, b0 = init_weights(best_mode, dim, seed_base)\n",
    "        result = train_perceptron(X_train, y_train, w0, b0, lr=0.1, theta=theta, max_epochs=max_epochs)\n",
    "\n",
    "        y_pred_test = np.where(np.dot(X_test, result[\"w\"]) + result[\"b\"] >= theta, 1, -1)\n",
    "        test_err = np.mean(y_pred_test != y_test)\n",
    "        theta_results.append((theta, test_err, result))\n",
    "\n",
    "        print(f\"θ={theta:+.2f} → test error={test_err*100:.2f}% conv={result['converged_epoch']}\")\n",
    "\n",
    "    best_theta, best_theta_err, best_theta_result = min(theta_results, key=lambda x: x[1])\n",
    "    print(f\"\\n✅ Best θ: {best_theta:+.2f} (error={best_theta_err*100:.2f}%)\")\n",
    "\n",
    "    # -------------------------\n",
    "    # (C) Learning Rate Stage\n",
    "    # -------------------------\n",
    "    print(\"\\n=== Stage C: Evaluating learning rates (η) ===\")\n",
    "    eta_results = []\n",
    "    for eta in eta_list:\n",
    "        w0, b0 = init_weights(best_mode, dim, seed_base)\n",
    "        result = train_perceptron(X_train, y_train, w0, b0, lr=eta, theta=best_theta, max_epochs=max_epochs)\n",
    "\n",
    "        y_pred_test = np.where(np.dot(X_test, result[\"w\"]) + result[\"b\"] >= best_theta, 1, -1)\n",
    "        test_err = np.mean(y_pred_test != y_test)\n",
    "        eta_results.append((eta, test_err, result))\n",
    "\n",
    "        print(f\"η={eta:.3f} → test error={test_err*100:.2f}% conv={result['converged_epoch']}\")\n",
    "\n",
    "    best_eta, best_eta_err, best_eta_result = min(eta_results, key=lambda x: x[1])\n",
    "    print(f\"\\n✅ Best η: {best_eta} (error={best_eta_err*100:.2f}%)\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Final Summary\n",
    "    # -------------------------\n",
    "    best_summary = {\n",
    "        \"best_init\": best_mode,\n",
    "        \"best_theta\": best_theta,\n",
    "        \"best_eta\": best_eta,\n",
    "        \"final_test_error\": best_eta_err,\n",
    "        \"converged_epoch\": best_eta_result[\"converged_epoch\"],\n",
    "        \"elapsed\": best_eta_result[\"elapsed\"]\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== Final Best Configuration ===\")\n",
    "    for k, v in best_summary.items():\n",
    "        print(f\"{k:15s}: {v}\")\n",
    "    \n",
    "    return best_summary, {\n",
    "        \"init_results\": init_results,\n",
    "        \"theta_results\": theta_results,\n",
    "        \"eta_results\": eta_results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57d8bb",
   "metadata": {},
   "source": [
    "## Running the Experiment\n",
    "Now we can run the full pipeline and observe the best initialization, threshold, and learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54837e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A: Finding best initialization mode ===\n",
      "zeros   → test error=4.76% conv=3\n",
      "uniform → test error=14.29% conv=3\n",
      "normal  → test error=0.00% conv=3\n",
      "xavier  → test error=0.00% conv=3\n",
      "kaiming → test error=0.00% conv=3\n",
      "\n",
      "✅ Best init mode: normal (error=0.00%)\n",
      "\n",
      "=== Stage B: Finding best threshold (θ) ===\n",
      "θ=-1.00 → test error=0.00% conv=3\n",
      "θ=-0.50 → test error=9.52% conv=3\n",
      "θ=+0.00 → test error=0.00% conv=3\n",
      "θ=+0.50 → test error=4.76% conv=4\n",
      "θ=+1.00 → test error=9.52% conv=3\n",
      "\n",
      "✅ Best θ: -1.00 (error=0.00%)\n",
      "\n",
      "=== Stage C: Evaluating learning rates (η) ===\n",
      "η=0.010 → test error=19.05% conv=4\n",
      "η=0.050 → test error=4.76% conv=3\n",
      "η=0.100 → test error=0.00% conv=3\n",
      "η=0.300 → test error=0.00% conv=5\n",
      "\n",
      "✅ Best η: 0.1 (error=0.00%)\n",
      "\n",
      "=== Final Best Configuration ===\n",
      "best_init      : normal\n",
      "best_theta     : -1.0\n",
      "best_eta       : 0.1\n",
      "final_test_error: 0.0\n",
      "converged_epoch: 3\n",
      "elapsed        : 0.0\n"
     ]
    }
   ],
   "source": [
    "best_summary, all_results = experiment_stagewise(\n",
    "    train_folder=\"Characters-TrainSet\",\n",
    "    test_folder=\"Characters-TestSet\",\n",
    "    max_epochs=30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f093bb13",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- **Best Init Mode:** chosen from five initialization methods.\n",
    "- **Best θ:** selected based on minimal test error using the chosen init.\n",
    "- **Best η:** chosen with the previous best parameters.\n",
    "- The final summary shows total training time, convergence epoch, and test error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c48502",
   "metadata": {},
   "source": [
    "Ensure we have the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ab6527a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing best_summary: {'best_init': 'normal', 'best_theta': -1.0, 'best_eta': 0.1, 'final_test_error': np.float64(0.0), 'converged_epoch': 3, 'elapsed': 0.0}\n",
      "\n",
      "Best config (used for next steps): normal -1.0 0.1\n"
     ]
    }
   ],
   "source": [
    "# Try to use existing best_summary; if not present, compute it (this may take time)\n",
    "try:\n",
    "    best_summary  # if defined in notebook previously\n",
    "    print(\"Found existing best_summary:\", best_summary)\n",
    "except NameError:\n",
    "    print(\"best_summary not found — running stagewise search (this may take some time)...\")\n",
    "    best_summary, all_results = experiment_stagewise(\n",
    "        train_folder=\"Characters-TrainSet\",\n",
    "        test_folder=\"Characters-TestSet\",\n",
    "        max_epochs=30\n",
    "    )\n",
    "\n",
    "# Extract best params\n",
    "best_init = best_summary[\"best_init\"]\n",
    "best_theta = best_summary[\"best_theta\"]\n",
    "best_eta = best_summary[\"best_eta\"]\n",
    "print(\"\\nBest config (used for next steps):\", best_init, best_theta, best_eta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf516f13",
   "metadata": {},
   "source": [
    "Important functions for this section (train perceptron already available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab6469df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(w, b, X, y, theta):\n",
    "    \"\"\"Return error rate and accuracy for given weights/bias and threshold.\"\"\"\n",
    "    preds = np.where(np.dot(X, w) + b >= theta, 1, -1)\n",
    "    err = np.mean(preds != y)\n",
    "    acc = accuracy_score(y, preds)\n",
    "    return {\"error\": err, \"accuracy\": acc, \"preds\": preds}\n",
    "\n",
    "# -------------------------\n",
    "# Adaline (Batch Gradient Descent) - manual implementation\n",
    "# -------------------------\n",
    "def train_adaline_batch(X, y, w_init, b_init, lr=0.01, max_epochs=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Batch gradient descent Adaline that minimizes MSE:\n",
    "      z = w·x + b\n",
    "      error = y - z\n",
    "      MSE = mean(error^2)\n",
    "    Update (batch):\n",
    "      w <- w + lr * mean((y - z) * x)  (note: sign depends on convention)\n",
    "      b <- b + lr * mean(y - z)\n",
    "    Returns dict with final w,b, mse_list, elapsed, converged_epoch\n",
    "    \"\"\"\n",
    "    w = w_init.copy().astype(float)\n",
    "    b = float(b_init)\n",
    "    N = X.shape[0]\n",
    "    mse_list = []\n",
    "    start = time.time()\n",
    "    converged_epoch = None\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        z = X.dot(w) + b               # shape (N,)\n",
    "        errors = y - z                 # shape (N,)\n",
    "        mse = np.mean(errors**2)\n",
    "        mse_list.append(mse)\n",
    "\n",
    "        # gradient of 0.5*MSE wrt w is -mean(error * x) if MSE = mean((y - z)^2)\n",
    "        # We want w <- w + lr * mean(error * x)\n",
    "        grad_w = np.mean((errors)[:, None] * X, axis=0)  # shape (D,)\n",
    "        grad_b = np.mean(errors)\n",
    "\n",
    "        w += lr * grad_w\n",
    "        b += lr * grad_b\n",
    "\n",
    "        if epoch > 1 and abs(mse_list[-2] - mse_list[-1]) < tol:\n",
    "            converged_epoch = epoch\n",
    "            break\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    return {\"w\": w, \"b\": b, \"mse_list\": mse_list,\n",
    "            \"converged_epoch\": converged_epoch or max_epochs,\n",
    "            \"elapsed\": elapsed}\n",
    "\n",
    "# -------------------------\n",
    "# Adaline via sklearn.SGDRegressor wrapper (just train & return continuous regressor)\n",
    "# -------------------------\n",
    "def train_adaline_sklearn(X, y, learning_rate=0.01, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Train sklearn SGDRegressor with squared loss; targets are expected as -1/+1.\n",
    "    Returns trained regressor and elapsed time.\n",
    "    \"\"\"\n",
    "    reg = SGDRegressor(loss='squared_error', learning_rate='constant', eta0=learning_rate,\n",
    "                       max_iter=max_iter, tol=1e-6, random_state=42)\n",
    "    start = time.time()\n",
    "    reg.fit(X, y)\n",
    "    elapsed = time.time() - start\n",
    "    return reg, elapsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron + Adaline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Perceptron Results ===\n",
      "Converged Epoch: 3 | Time: 0.0000s\n",
      "Train Accuracy: 100.00% | Test Accuracy: 100.00%\n",
      "\n",
      "=== Adaline Results ===\n",
      "Epochs: 200 | Time: 0.0125s\n",
      "Train Accuracy: 66.67% | Test Accuracy: 33.33%\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Train Accuracy (%)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test Accuracy (%)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Epochs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Time (s)",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "adfd146c-668d-4876-8a95-48148dce030a",
       "rows": [
        [
         "0",
         "Perceptron",
         "100.0",
         "100.0",
         "3",
         "0.0"
        ],
        [
         "1",
         "Adaline",
         "66.667",
         "33.333",
         "200",
         "0.013"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy (%)</th>\n",
       "      <th>Test Accuracy (%)</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaline</td>\n",
       "      <td>66.667</td>\n",
       "      <td>33.333</td>\n",
       "      <td>200</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Train Accuracy (%)  Test Accuracy (%)  Epochs  Time (s)\n",
       "0  Perceptron             100.000            100.000       3     0.000\n",
       "1     Adaline              66.667             33.333     200     0.013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Load dataset again ===\n",
    "X_train, y_train, _ = load_dataset(\"Characters-TrainSet\")\n",
    "X_test, y_test, _ = load_dataset(\"Characters-TestSet\")\n",
    "\n",
    "# === Use best hyperparameters from previous experiment ===\n",
    "best_init = \"normal\"\n",
    "best_theta = -1.0\n",
    "best_eta = 0.1\n",
    "\n",
    "# Initialize weights reproducibly\n",
    "dim = X_train.shape[1]\n",
    "seed = 999\n",
    "w0, b0 = init_weights(best_init, dim, seed)\n",
    "\n",
    "# === Train Perceptron ===\n",
    "perc_res = train_perceptron(X_train, y_train, w0, b0, lr=best_eta, theta=best_theta, max_epochs=200)\n",
    "\n",
    "# Evaluation helper\n",
    "def evaluate_model(w, b, X, y, theta):\n",
    "    preds = np.where(np.dot(X, w) + b >= theta, 1, -1)\n",
    "    accuracy = np.mean(preds == y)\n",
    "    error = 1 - accuracy\n",
    "    return {\"accuracy\": accuracy, \"error\": error}\n",
    "\n",
    "perc_train_eval = evaluate_model(perc_res[\"w\"], perc_res[\"b\"], X_train, y_train, best_theta)\n",
    "perc_test_eval  = evaluate_model(perc_res[\"w\"], perc_res[\"b\"], X_test,  y_test,  best_theta)\n",
    "\n",
    "print(\"=== Perceptron Results ===\")\n",
    "print(f\"Converged Epoch: {perc_res['converged_epoch']} | Time: {perc_res['elapsed']:.4f}s\")\n",
    "print(f\"Train Accuracy: {perc_train_eval['accuracy']*100:.2f}% | Test Accuracy: {perc_test_eval['accuracy']*100:.2f}%\")\n",
    "\n",
    "# ============================================================\n",
    "#                   Adaline implementation\n",
    "# ============================================================\n",
    "\n",
    "def train_adaline(X, y, w_init, b_init, lr=0.1, theta=0.0, max_epochs=100):\n",
    "    w, b = w_init.copy(), float(b_init)\n",
    "    errors, losses = [], []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # Linear output (no step function)\n",
    "        y_net = np.dot(X, w) + b\n",
    "        error = y - y_net\n",
    "        w += lr * np.dot(X.T, error) / len(X)\n",
    "        b += lr * np.mean(error)\n",
    "\n",
    "        # Mean squared error\n",
    "        mse = np.mean(error**2)\n",
    "        losses.append(mse)\n",
    "\n",
    "        # convergence criterion\n",
    "        if mse < 1e-4:\n",
    "            break\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    return {\"w\": w, \"b\": b, \"losses\": losses, \"epochs\": epoch, \"elapsed\": elapsed}\n",
    "\n",
    "# Train Adaline\n",
    "w0, b0 = init_weights(best_init, dim, seed)\n",
    "adaline_res = train_adaline(X_train, y_train, w0, b0, lr=best_eta, theta=best_theta, max_epochs=200)\n",
    "\n",
    "# Evaluate Adaline\n",
    "adaline_train_eval = evaluate_model(adaline_res[\"w\"], adaline_res[\"b\"], X_train, y_train, best_theta)\n",
    "adaline_test_eval  = evaluate_model(adaline_res[\"w\"], adaline_res[\"b\"], X_test,  y_test,  best_theta)\n",
    "\n",
    "print(\"\\n=== Adaline Results ===\")\n",
    "print(f\"Epochs: {adaline_res['epochs']} | Time: {adaline_res['elapsed']:.4f}s\")\n",
    "print(f\"Train Accuracy: {adaline_train_eval['accuracy']*100:.2f}% | Test Accuracy: {adaline_test_eval['accuracy']*100:.2f}%\")\n",
    "\n",
    "# ============================================================\n",
    "#                   Comparison Summary\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Perceptron\",\n",
    "        \"Train Accuracy (%)\": perc_train_eval[\"accuracy\"]*100,\n",
    "        \"Test Accuracy (%)\": perc_test_eval[\"accuracy\"]*100,\n",
    "        \"Epochs\": perc_res[\"converged_epoch\"],\n",
    "        \"Time (s)\": perc_res[\"elapsed\"]\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Adaline\",\n",
    "        \"Train Accuracy (%)\": adaline_train_eval[\"accuracy\"]*100,\n",
    "        \"Test Accuracy (%)\": adaline_test_eval[\"accuracy\"]*100,\n",
    "        \"Epochs\": adaline_res[\"epochs\"],\n",
    "        \"Time (s)\": adaline_res[\"elapsed\"]\n",
    "    }\n",
    "])\n",
    "\n",
    "display(summary.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19bc658",
   "metadata": {},
   "source": [
    "feature projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3bed14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (21, 16)\n",
      "Example feature vector:\n",
      " [2. 1. 1. 2. 2. 5. 2. 2. 6. 1. 4. 5. 4. 4. 4. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_and_project(folder):\n",
    "    \"\"\"\n",
    "    Reads all character text files from a folder and returns\n",
    "    the projected feature matrix (row+col sums) and labels.\n",
    "    \"\"\"\n",
    "    X, y, names = [], [], []\n",
    "    for fname in sorted(os.listdir(folder)):\n",
    "        if not fname.endswith(\".txt\"):\n",
    "            continue\n",
    "        path = os.path.join(folder, fname)\n",
    "        label = fname[0].upper()  # e.g., 'A1.txt' → 'A'\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = [list(line.strip()) for line in f.readlines() if line.strip()]\n",
    "        # Convert to binary 0/1\n",
    "        arr = np.array([[1 if ch == \"#\" else 0 for ch in row] for row in lines])\n",
    "        # Compute row & column projections\n",
    "        row_sum = np.sum(arr, axis=1)\n",
    "        col_sum = np.sum(arr, axis=0)\n",
    "        features = np.concatenate([row_sum, col_sum])\n",
    "        X.append(features)\n",
    "        y.append(label)\n",
    "        names.append(fname)\n",
    "    return np.array(X, dtype=float), np.array(y), names\n",
    "\n",
    "# Example usage:\n",
    "X_proj_train, y_proj_train, _ = load_and_project(\"Characters-TrainSet\")\n",
    "X_proj_test, y_proj_test, _ = load_and_project(\"Characters-TestSet\")\n",
    "\n",
    "print(\"Train shape:\", X_proj_train.shape)\n",
    "print(\"Example feature vector:\\n\", X_proj_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1c8f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Encode class labels (A → 0, B → 1, …)\n",
    "enc = LabelEncoder()\n",
    "y_train_enc = enc.fit_transform(y_proj_train)\n",
    "y_test_enc  = enc.transform(y_proj_test)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_proj_train)\n",
    "X_test_s  = scaler.transform(X_proj_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "630b0e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Projected Features): 61.90%\n",
      "Confusion Matrix:\n",
      " [[2 0 0 0 1 0 0]\n",
      " [0 2 0 0 1 0 0]\n",
      " [0 0 2 0 1 0 0]\n",
      " [0 0 0 2 0 1 0]\n",
      " [0 0 0 0 3 0 0]\n",
      " [0 0 0 0 3 0 0]\n",
      " [0 0 0 0 1 0 2]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         3\n",
      "           1       1.00      0.67      0.80         3\n",
      "           2       1.00      0.67      0.80         3\n",
      "           3       1.00      0.67      0.80         3\n",
      "           4       0.30      1.00      0.46         3\n",
      "           5       0.00      0.00      0.00         3\n",
      "           6       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.62        21\n",
      "   macro avg       0.76      0.62      0.64        21\n",
      "weighted avg       0.76      0.62      0.64        21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Train Perceptron\n",
    "perc_proj = Perceptron(max_iter=1000, eta0=0.1, random_state=42)\n",
    "perc_proj.fit(X_train_s, y_train_enc)\n",
    "\n",
    "# Predict\n",
    "y_pred_proj = perc_proj.predict(X_test_s)\n",
    "\n",
    "# Evaluate\n",
    "acc_proj = accuracy_score(y_test_enc, y_pred_proj)\n",
    "print(f\"Accuracy (Projected Features): {acc_proj*100:.2f}%\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_enc, y_pred_proj))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_enc, y_pred_proj, zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

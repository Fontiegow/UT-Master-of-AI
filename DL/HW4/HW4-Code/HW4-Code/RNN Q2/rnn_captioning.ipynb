{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDJwQPZcupab"
   },
   "source": [
    "#Assignment 4: Image captioning with RNNs\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Farid Nahaie, #159404027   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Euek3FWn6bhA",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Image Captioning with RNNs\n",
    "\n",
    "In this exercise, you will implement vanilla recurrent neural networks (RNNs) to train a model that can generate natural language captions for images.\n",
    "\n",
    "Models in this exercise are highly similar to very early works in neural-network based image captioning. If you are interested to learn more, check out these two papers:\n",
    "\n",
    "1. [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555)\n",
    "2. [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzqbYcKdz6ew"
   },
   "source": [
    "## Setup Code\n",
    "\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as previous assignments. You\"ll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1701244095108,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "28O_qwFfdQpr"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3H9pcnudWlg"
   },
   "source": [
    "### Google Colab Setup\n",
    "\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33672,
     "status": "ok",
     "timestamp": 1701244128768,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "Yv8Z8EiudX25",
    "outputId": "fbfcbca1-950a-4766-8385-8a460d7624d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sek0GtVOdlKT"
   },
   "source": [
    " Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "[\"common\", \"a5_helper.py\", \"rnn_captioning.ipynb\",  \"rnn_captioning.py\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 592,
     "status": "ok",
     "timestamp": 1701244136714,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "A9t0-bGZdnr8",
    "outputId": "974e7aee-c589-4307-9d82-7bc602c78f9b"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'drive\\\\My Drive\\\\RNN'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \u001b[33m'\u001b[39m\u001b[33mRNN\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      7\u001b[39m GOOGLE_DRIVE_PATH = os.path.join(\u001b[33m\"\u001b[39m\u001b[33mdrive\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMy Drive\u001b[39m\u001b[33m\"\u001b[39m, GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGOOGLE_DRIVE_PATH\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Add to sys so we can import .py files.\u001b[39;00m\n\u001b[32m     12\u001b[39m sys.path.append(GOOGLE_DRIVE_PATH)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'drive\\\\My Drive\\\\RNN'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a WI2022 folder and put all the files under A5 folder, then \"WI2022/A5\"\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'RNN'\n",
    "GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "\n",
    "\n",
    "# Add to sys so we can import .py files.\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5LWJPBtdrpZ"
   },
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from rnn_captioning.py!\n",
    "```\n",
    "\n",
    "as well as the last edit time for the file `rnn_captioning.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6167,
     "status": "ok",
     "timestamp": 1701244146712,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "jFZKi0podzhO",
    "outputId": "5b4d212d-7620-4c11-f785-2a3b7cdf3be1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from rnn_captioning import hello_rnn_captioning\n",
    "\n",
    "\n",
    "os.environ[\"TZ\"] = \"Asia/Tehran\"\n",
    "time.tzset()\n",
    "hello_rnn_captioning()\n",
    "\n",
    "rnn_path = os.path.join(GOOGLE_DRIVE_PATH, \"rnn_captioning.py\")\n",
    "rnn_edit_ime = time.ctime(os.path.getmtime(rnn_path))\n",
    "print(\"rnn_captioning.py last edited on %s\" % rnn_edit_ime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8fTwRpXfwyM"
   },
   "source": [
    "### Load Packages\n",
    "\n",
    "Run some setup code for this notebook: Import some useful packages and increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4718,
     "status": "ok",
     "timestamp": 1701244171715,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "q53DlMXboP-T",
    "outputId": "e36438c0-c401-4186-8946-6ede221ae2d8"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from helpers_module.grad import compute_numeric_gradient, rel_error\n",
    "from helpers_module.utils import attention_visualizer, reset_seed\n",
    "\n",
    "# plotting setup\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_theme(\n",
    "    style=\"darkgrid\",\n",
    "    context=\"talk\",\n",
    "    rc={\n",
    "        \"figure.figsize\": (10.0, 8.0),\n",
    "        \"image.interpolation\": \"nearest\",\n",
    "        \"image.cmap\": \"gray\",\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvUDZWGU3VLV"
   },
   "source": [
    "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1701244191353,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "RrAX9FOLpr9k",
    "outputId": "828b3965-0eed-4591-a416-6ffc1cf5eb06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# Define some common variables for dtypes/devices.\n",
    "# These can be keyword arguments while defining new tensors.\n",
    "to_float = {\"dtype\": torch.float32, \"device\": DEVICE}\n",
    "to_double = {\"dtype\": torch.float64, \"device\": DEVICE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCPZwvOd6bhF",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# COCO Captions\n",
    "\n",
    "For this exercise we will use the 2014 release of the [COCO Captions dataset](http://cocodataset.org/) which has become the standard testbed for image captioning. The dataset consists of 80,000 training images and 40,000 validation images, each annotated with 5 captions written by workers on Amazon Mechanical Turk.\n",
    "\n",
    "We have preprocessed the data for you already and saved them into a serialized data file. It contains 10,000 image-caption pairs for training and 500 for testing. The images have been downsampled to 112x112 for computation efficiency and captions are tokenized and numericalized, clamped to 15 words. You can download the file named `coco.pt` (378MB) with the link below and run some useful stats.\n",
    "\n",
    "You will later use RegNet-X 400MF model to extract features for the images. A few notes on the caption preprocessing:\n",
    "\n",
    "Dealing with strings is inefficient, so we will work with an encoded version of the captions. Each word is assigned an integer ID, allowing us to represent a caption by a sequence of integers. The mapping between integer IDs and words is saved in an entry named `vocab` (both `idx_to_token` and `token_to_idx`), and we use the function `decode_captions` from `a5_helper.py` to convert tensors of integer IDs back into strings.\n",
    "\n",
    "There are a couple special tokens that we add to the vocabulary. We prepend a special `<START>` token and append an `<END>` token to the beginning and end of each caption respectively. Rare words are replaced with a special `<UNK>` token (for \"unknown\"). In addition, since we want to train with minibatches containing captions of different lengths, we pad short captions with a special `<NULL>` token after the `<END>` token and don't compute loss or gradient for `<NULL>` tokens. Since they are a bit of a pain, we have taken care of all implementation details around special tokens for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1701244194608,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "Ge6TOSulapq7"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Set a few constants related to data loading.\n",
    "IMAGE_SHAPE = (112, 112)\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "\n",
    "# Batch size used for full training runs:\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Batch size used for overfitting sanity checks:\n",
    "OVR_BATCH_SIZE = BATCH_SIZE // 8\n",
    "\n",
    "# Batch size used for visualization:\n",
    "VIS_BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11034,
     "status": "ok",
     "timestamp": 1701244206272,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "IMok4gFXqjre",
    "outputId": "4994a9de-dfa5-4812-f45d-628a369e3551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading COCO dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/coco.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mwget http://web.eecs.umich.edu/~justincj/teaching/eecs498/coco.pt -P ./datasets/\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# load COCO data from coco.pt, loaf_COCO is implemented in a5_helper.py\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m data_dict = \u001b[43mload_coco_captions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./datasets/coco.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m num_train = data_dict[\u001b[33m\"\u001b[39m\u001b[33mtrain_images\u001b[39m\u001b[33m\"\u001b[39m].size(\u001b[32m0\u001b[39m)\n\u001b[32m     21\u001b[39m num_val = data_dict[\u001b[33m\"\u001b[39m\u001b[33mval_images\u001b[39m\u001b[33m\"\u001b[39m].size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\DL\\HW4\\HW4-Code\\HW4-Code\\RNN Q2\\a5_helper.py:22\u001b[39m, in \u001b[36mload_coco_captions\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_coco_captions\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m./datasets/coco.pt\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     11\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m    Download and load serialized COCO data from coco.pt\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m    It contains a dictionary of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[33;03m    Returns: a data dictionary\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     data_dict = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# print out all the keys and values from the data dictionary\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data_dict.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\venv\\Lib\\site-packages\\torch\\serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\venv\\Lib\\site-packages\\torch\\serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\venv\\Lib\\site-packages\\torch\\serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './datasets/coco.pt'"
     ]
    }
   ],
   "source": [
    "from a5_helper import load_coco_captions\n",
    "\n",
    "# Download and load serialized COCO data from coco.pt\n",
    "# It contains a dictionary of\n",
    "# \"train_images\" - resized training images (IMAGE_SHAPE)\n",
    "# \"val_images\" - resized validation images (IMAGE_SHAPE)\n",
    "# \"train_captions\" - tokenized and numericalized training captions\n",
    "# \"val_captions\" - tokenized and numericalized validation captions\n",
    "# \"vocab\" - caption vocabulary, including \"idx_to_token\" and \"token_to_idx\"\n",
    "\n",
    "if os.path.isfile(\"./datasets/coco.pt\"):\n",
    "    print(\"COCO data exists!\")\n",
    "else:\n",
    "    print(\"downloading COCO dataset\")\n",
    "    !wget http://web.eecs.umich.edu/~justincj/teaching/eecs498/coco.pt -P ./datasets/\n",
    "\n",
    "# load COCO data from coco.pt, loaf_COCO is implemented in a5_helper.py\n",
    "data_dict = load_coco_captions(path=\"./datasets/coco.pt\")\n",
    "\n",
    "num_train = data_dict[\"train_images\"].size(0)\n",
    "num_val = data_dict[\"val_images\"].size(0)\n",
    "\n",
    "# declare variables for special tokens\n",
    "NULL_index = data_dict[\"vocab\"][\"token_to_idx\"][\"<NULL>\"]\n",
    "START_index = data_dict[\"vocab\"][\"token_to_idx\"][\"<START>\"]\n",
    "END_index = data_dict[\"vocab\"][\"token_to_idx\"][\"<END>\"]\n",
    "UNK_index = data_dict[\"vocab\"][\"token_to_idx\"][\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80RW_nSH6bhH"
   },
   "source": [
    "## Look at the data\n",
    "It is always a good idea to look at examples from the dataset before working with it.\n",
    "\n",
    "Run the following to sample a small minibatch of training data and show the images and their captions. Running it multiple times and looking at the results helps you to get a sense of the dataset.\n",
    "\n",
    "Note that we decode the captions using the `decode_captions` function.\n",
    "You can check its implementation in `a5_helper.py`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1476,
     "status": "ok",
     "timestamp": 1701244207719,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "l-oiW9Ut6bhH",
    "outputId": "bb43447c-f2a1-484d-fd2b-cc09e7d953d7"
   },
   "outputs": [],
   "source": [
    "from a5_helper import decode_captions\n",
    "\n",
    "\n",
    "# Sample a minibatch and show the reshaped 112x112 images and captions\n",
    "sample_idx = torch.randint(0, num_train, (VIS_BATCH_SIZE, ))\n",
    "sample_images = data_dict[\"train_images\"][sample_idx]\n",
    "sample_captions = data_dict[\"train_captions\"][sample_idx]\n",
    "for i in range(VIS_BATCH_SIZE):\n",
    "    plt.imshow(sample_images[i].permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    caption_str = decode_captions(\n",
    "        sample_captions[i], data_dict[\"vocab\"][\"idx_to_token\"]\n",
    "    )\n",
    "    plt.title(caption_str)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2SQMNIH6bhJ"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "As discussed in lecture, we will use Recurrent Neural Network (RNN) language models for image captioning. We will cover the vanilla RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XHZMI356bhJ"
   },
   "source": [
    "## Vanilla RNN: step forward\n",
    "\n",
    "First implement the `rnn_step_forward` for a single timestep of a vanilla recurrent neural network.\n",
    "Run the following to check your implementation. You should see errors on the order of `1e-8` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8401,
     "status": "ok",
     "timestamp": 1701244297600,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "c3oU8JJj6bhK",
    "outputId": "7d157db7-5b64-4449-c6ac-52c1370e515f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_h error:  2.3200594408551194e-09\n"
     ]
    }
   ],
   "source": [
    "from rnn_captioning import rnn_step_forward\n",
    "\n",
    "N, D, H = 3, 10, 4\n",
    "\n",
    "x = torch.linspace(-0.4, 0.7, steps=N * D, **to_double).view(N, D)\n",
    "prev_h = torch.linspace(-0.2, 0.5, steps=N * H, **to_double).view(N, H)\n",
    "Wx = torch.linspace(-0.1, 0.9, steps=D * H, **to_double).view(D, H)\n",
    "Wh = torch.linspace(-0.3, 0.7, steps=H * H, **to_double).view(H, H)\n",
    "b = torch.linspace(-0.2, 0.4, steps=H, **to_double)\n",
    "\n",
    "\n",
    "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "expected_next_h = torch.tensor(\n",
    "    [\n",
    "        [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "        [0.66854692, 0.79562378, 0.87755553, 0.92795967],\n",
    "        [0.97934501, 0.99144213, 0.99646691, 0.99854353],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"next_h error: \", rel_error(expected_next_h, next_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tid-ljPA6bhL"
   },
   "source": [
    "## Vanilla RNN: step backward\n",
    "Then implement the `rnn_step_backward` for a single timestep of a vanilla recurrent neural network. Run the following to numerically gradient check your implementation. You should see errors on the order of `1e-8` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 732,
     "status": "ok",
     "timestamp": 1701244325845,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "KPyfJofC6bhM",
    "outputId": "3c0c529a-5de0-4c1d-ce42-f068d44e7613"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'Tensor' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# YOUR_TURN: Implement rnn_step_backward\u001b[39;00m\n\u001b[32m     28\u001b[39m dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdx error: \u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mrel_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdx_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdprev_h error: \u001b[39m\u001b[33m\"\u001b[39m, rel_error(dprev_h_num, dprev_h))\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdWx error: \u001b[39m\u001b[33m\"\u001b[39m, rel_error(dWx_num, dWx))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\DL\\HW4\\HW4-Code\\HW4-Code\\RNN Q2\\helpers_module\\grad.py:120\u001b[39m, in \u001b[36mrel_error\u001b[39m\u001b[34m(x, y, eps)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03mCompute the relative error between a pair of tensors x and y,\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03mwhich is defined as:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m- rel_error: Scalar giving the relative error between x and y\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" returns relative error between x and y \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m top = (\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m).abs().max().item()\n\u001b[32m    121\u001b[39m bot = (x.abs() + y.abs()).clamp(\u001b[38;5;28mmin\u001b[39m=eps).max().item()\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m top / bot\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for -: 'Tensor' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from rnn_captioning import rnn_step_backward\n",
    "reset_seed(0)\n",
    "\n",
    "N, D, H = 4, 5, 6\n",
    "x = torch.randn(N, D, **to_double)\n",
    "h = torch.randn(N, H, **to_double)\n",
    "Wx = torch.randn(D, H, **to_double)\n",
    "Wh = torch.randn(H, H, **to_double)\n",
    "b = torch.randn(H, **to_double)\n",
    "\n",
    "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
    "\n",
    "dnext_h = torch.randn(*out.shape, **to_double)\n",
    "\n",
    "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fh = lambda h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = compute_numeric_gradient(fx, x, dnext_h)\n",
    "dprev_h_num = compute_numeric_gradient(fh, h, dnext_h)\n",
    "dWx_num = compute_numeric_gradient(fWx, Wx, dnext_h)\n",
    "dWh_num = compute_numeric_gradient(fWh, Wh, dnext_h)\n",
    "db_num = compute_numeric_gradient(fb, b, dnext_h)\n",
    "\n",
    "# YOUR_TURN: Implement rnn_step_backward\n",
    "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
    "\n",
    "print(\"dx error: \", rel_error(dx_num, dx))\n",
    "print(\"dprev_h error: \", rel_error(dprev_h_num, dprev_h))\n",
    "print(\"dWx error: \", rel_error(dWx_num, dWx))\n",
    "print(\"dWh error: \", rel_error(dWh_num, dWh))\n",
    "print(\"db error: \", rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjZjH5JW6bhN"
   },
   "source": [
    "## Vanilla RNN: forward\n",
    "Now that you have implemented the forward and backward passes for a single timestep of a vanilla RNN, you will combine these pieces to implement a RNN that processes an entire sequence of data. First implement `rnn_forward` by making calls to the `rnn_step_forward` function that you defined earlier.\n",
    "\n",
    "Run the following to check your implementation. You should see errors on the order of `1e-6` or less.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1701244385744,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "_GQWEn3Z6bhO",
    "outputId": "6995bae5-836c-46fc-f5df-91402fe28fbe"
   },
   "outputs": [],
   "source": [
    "from rnn_captioning import rnn_forward\n",
    "\n",
    "\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "\n",
    "x = torch.linspace(-0.1, 0.3, steps=N * T * D, **to_double).view(N, T, D)\n",
    "h0 = torch.linspace(-0.3, 0.1, steps=N * H, **to_double).view(N, H)\n",
    "Wx = torch.linspace(-0.2, 0.4, steps=D * H, **to_double).view(D, H)\n",
    "Wh = torch.linspace(-0.4, 0.1, steps=H * H, **to_double).view(H, H)\n",
    "b = torch.linspace(-0.7, 0.1, steps=H, **to_double)\n",
    "\n",
    "# YOUR_TURN: Implement rnn_forward\n",
    "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "expected_h = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [-0.42070749, -0.27279261, -0.11074945, 0.05740409, 0.22236251],\n",
    "            [-0.39525808, -0.22554661, -0.0409454, 0.14649412, 0.32397316],\n",
    "            [-0.42305111, -0.24223728, -0.04287027, 0.15997045, 0.35014525],\n",
    "        ],\n",
    "        [\n",
    "            [-0.55857474, -0.39065825, -0.19198182, 0.02378408, 0.23735671],\n",
    "            [-0.27150199, -0.07088804, 0.13562939, 0.33099728, 0.50158768],\n",
    "            [-0.51014825, -0.30524429, -0.06755202, 0.17806392, 0.40333043],\n",
    "        ],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "print(\"h error: \", rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P570PTsw6bhP"
   },
   "source": [
    "## Vanilla RNN: backward\n",
    "Implement the `rnn_backward` for a vanilla RNN. This should run back-propagation over the entire sequence, making calls to the `rnn_step_backward` function that you defined earlier. You should see errors on the order of `1e-6` or less.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1291,
     "status": "ok",
     "timestamp": 1701244414868,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "Ny25RusA6bhQ",
    "outputId": "648b3147-b6dd-4901-bed0-68dec34edaba"
   },
   "outputs": [],
   "source": [
    "from rnn_captioning import rnn_backward, rnn_forward\n",
    "reset_seed(0)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "x = torch.randn(N, T, D, **to_double)\n",
    "h0 = torch.randn(N, H, **to_double)\n",
    "Wx = torch.randn(D, H, **to_double)\n",
    "Wh = torch.randn(H, H, **to_double)\n",
    "b = torch.randn(H, **to_double)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = torch.randn(*out.shape, **to_double)\n",
    "\n",
    "# YOUR_TURN: Implement rnn_backward\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = compute_numeric_gradient(fx, x, dout)\n",
    "dh0_num = compute_numeric_gradient(fh0, h0, dout)\n",
    "dWx_num = compute_numeric_gradient(fWx, Wx, dout)\n",
    "dWh_num = compute_numeric_gradient(fWh, Wh, dout)\n",
    "db_num = compute_numeric_gradient(fb, b, dout)\n",
    "\n",
    "print(\"dx error: \", rel_error(dx_num, dx))\n",
    "print(\"dh0 error: \", rel_error(dh0_num, dh0))\n",
    "print(\"dWx error: \", rel_error(dWx_num, dWx))\n",
    "print(\"dWh error: \", rel_error(dWh_num, dWh))\n",
    "print(\"db error: \", rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEDUmZOkU_LO"
   },
   "source": [
    "## Vanilla RNN: backward with autograd\n",
    "\n",
    "Now we will entirely depend on the PyTorch autograd module (`torch.autograd`) to compute the backward pass of RNN.\n",
    "`torch.autograd` provides classes and functions implementing **automatic differentiation** of arbitrary scalar valued functions.\n",
    "It requires minimal changes to the existing code - if you pass tensors with `requires_grad=True` to the forward function you wrote earlier, you can just call `.backward(gradient=grad)` on the output to compute gradients on the input and weights.\n",
    "\n",
    "\n",
    "\n",
    "Now we can compare the manual backward pass with the autograd backward pass.\n",
    "Read the code in following cell, and execute it to compare your implementation with `torch.autograd`.\n",
    "You should get a relative error less than `1e-10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1153,
     "status": "ok",
     "timestamp": 1701244425032,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "5AMXoqNOVRa_",
    "outputId": "cc21fd13-b731-47b6-e930-071bed5894dc"
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "# set requires_grad=True\n",
    "x = torch.randn(N, T, D, **to_double, requires_grad=True)\n",
    "h0 = torch.randn(N, H, **to_double, requires_grad=True)\n",
    "Wx = torch.randn(D, H, **to_double, requires_grad=True)\n",
    "Wh = torch.randn(H, H, **to_double, requires_grad=True)\n",
    "b = torch.randn(H, **to_double, requires_grad=True)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = torch.randn(*out.shape, **to_double)\n",
    "\n",
    "# Manual backward:\n",
    "with torch.no_grad():\n",
    "    dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "# Backward with autograd: the magic happens here!\n",
    "out.backward(dout)\n",
    "\n",
    "dx_auto, dh0_auto, dWx_auto, dWh_auto, db_auto = (\n",
    "    x.grad,\n",
    "    h0.grad,\n",
    "    Wx.grad,\n",
    "    Wh.grad,\n",
    "    b.grad,\n",
    ")\n",
    "\n",
    "print(\"dx error: \", rel_error(dx_auto, dx))\n",
    "print(\"dh0 error: \", rel_error(dh0_auto, dh0))\n",
    "print(\"dWx error: \", rel_error(dWx_auto, dWx))\n",
    "print(\"dWh error: \", rel_error(dWh_auto, dWh))\n",
    "print(\"db error: \", rel_error(db_auto, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgmxOjX0prA4"
   },
   "source": [
    "## RNN Module\n",
    "\n",
    "We can now wrap the vanilla RNN implementation into a PyTorch module.\n",
    "\n",
    "-- `nn.Module` is a base class for all neural network modules in PyTorch. More details regarding its attributes, functions, and methods could be found [in PyTorch documentation](https://pytorch.org/docs/stable/nn.html?highlight=module#torch.nn.Module).\n",
    "\n",
    "In short, the weights and biases are declared in `__init__` and function `forward` will call the `rnn_forward` function from before.\n",
    "The backward function will not be used, and entirely handled by `torch.autograd`.\n",
    "**We have written this part in `RNN` for you but you are highly recommended to go through the code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1273,
     "status": "ok",
     "timestamp": 1701244442627,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "h1Z1cQ8NaprG",
    "outputId": "74710d89-76b2-4859-b2db-1047eab0c75a"
   },
   "outputs": [],
   "source": [
    "from rnn_captioning import RNN, rnn_forward\n",
    "\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "x = torch.randn(N, T, D, **to_double)\n",
    "h0 = torch.randn(N, H, **to_double)\n",
    "\n",
    "rnn_module = RNN(D, H).to(**to_double)\n",
    "\n",
    "# Call forward in module:\n",
    "hn1 = rnn_module(x, h0)\n",
    "\n",
    "# Call without module: (but access weights from module)\n",
    "# Equivalent to above, we won't do this henceforth.\n",
    "Wx, Wh, b = rnn_module.Wx, rnn_module.Wh, rnn_module.b\n",
    "hn2, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "print(\"Output error with/without module: \", rel_error(hn1, hn2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIjmnjRd6bhZ"
   },
   "source": [
    "# RNN for image captioning\n",
    "\n",
    "You will implement a few necessary tools and layers in order to build an image captioning model (class `CaptioningRNN`).\n",
    "\n",
    "## Image Feature Extraction\n",
    "\n",
    "The first essential component in an image captioning model is an encoder that inputs an image and produces features for decoding the caption.\n",
    "Here, we use a small [RegNetX-400MF](https://pytorch.org/vision/stable/models.html#torchvision.models.regnet_x_400mf) as the backbone so we can train in reasonable time on Colab. This model is similar to detector backbone seen in the past assignment.\n",
    "\n",
    "It accepts image batches of shape `(B, C, H, W)` and outputs spatial features from final layer that have shape `(B, C, H/32, W/32)`.\n",
    "For vanilla RNN and LSTM, we use the average pooled features (shape `(B, C)`) for decoding captions, whereas for attention LSTM we aggregate the spatial features by learning attention weights.\n",
    "Checkout the `ImageEncoder` method in `rnn_lstm_captioning.py` to see the initialization of the model.\n",
    "\n",
    "We use the implementation from torchvision and put a very thin wrapper module for our use-case.\n",
    "You do not need to implement anything here â€” you should read and understand the module definition, available in `rnn_lstm_captioning.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 646,
     "status": "ok",
     "timestamp": 1701244455079,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "_pV0Lau_yDwX",
    "outputId": "e943ecdb-a4b9-466d-92a2-0dd6fb36dbb4"
   },
   "outputs": [],
   "source": [
    "from rnn_captioning import ImageEncoder\n",
    "\n",
    "model = ImageEncoder(pretrained=True, verbose=True).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVAxU-jO6bhR"
   },
   "source": [
    "## Word embedding\n",
    "In deep learning systems, we commonly represent words using vectors. Each word of the vocabulary will be associated with a vector, and these vectors will be learned jointly with the rest of the system.\n",
    "\n",
    "Implement the `WordEmbedding` module to convert words (represented by integers) into vectors.\n",
    "Run the following to check your implementation. You should see an error on the order of `1e-7` or less.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 670,
     "status": "ok",
     "timestamp": 1701244475905,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "BZuz2ieE6bhR",
    "outputId": "a0265ae4-cf13-4d0e-d734-7c062f9974c3"
   },
   "outputs": [],
   "source": [
    "from rnn_captioning import WordEmbedding\n",
    "\n",
    "N, T, V, D = 2, 4, 5, 3\n",
    "\n",
    "x = torch.tensor([[0, 3, 1, 2], [2, 1, 0, 3]]).long()\n",
    "W = torch.linspace(0, 1, steps=V * D, **to_double).view(V, D)\n",
    "\n",
    "# Copy custom weight vector for sanity check:\n",
    "model_emb = WordEmbedding(V, D).to(**to_double)\n",
    "model_emb.W_embed.data.copy_(W)\n",
    "out = model_emb(x)\n",
    "expected_out = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.0, 0.07142857, 0.14285714],\n",
    "            [0.64285714, 0.71428571, 0.78571429],\n",
    "            [0.21428571, 0.28571429, 0.35714286],\n",
    "            [0.42857143, 0.5, 0.57142857],\n",
    "        ],\n",
    "        [\n",
    "            [0.42857143, 0.5, 0.57142857],\n",
    "            [0.21428571, 0.28571429, 0.35714286],\n",
    "            [0.0, 0.07142857, 0.14285714],\n",
    "            [0.64285714, 0.71428571, 0.78571429],\n",
    "        ],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"out error: \", rel_error(expected_out, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6Py13Ak6bhX",
    "tags": []
   },
   "source": [
    "## Temporal Softmax loss\n",
    "\n",
    "In an RNN language model, at every timestep we produce a score for each word in the vocabulary.\n",
    "This score is obtained by applying an affine transform to the hidden state (think `nn.Linear` module).\n",
    "We know the ground-truth word at each timestep, so we use a cross-entropy loss at each timestep.\n",
    "We sum the losses over time and average them over the minibatch.\n",
    "\n",
    "However there is one wrinkle: since we operate over minibatches and different captions may have different lengths, we append `<NULL>` tokens to the end of each caption so they all have the same length. We don't want these `<NULL>` tokens to count toward the loss or gradient, so in addition to scores and ground-truth labels our loss function also accepts a `ignore_index` that tells it which index in caption should be ignored when computing the loss.\n",
    "\n",
    "Implement the `temporal_softmax_loss` and run the following cell to check if the implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1701244495499,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "nlFvgXtD6bhX",
    "outputId": "a5fea980-215e-4d89-a291-44524e246f4b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rnn_captioning import temporal_softmax_loss\n",
    "\n",
    "\n",
    "def check_loss(N, T, V, p):\n",
    "    x = 0.001 * torch.randn(N, T, V)\n",
    "    y = torch.randint(V, size=(N, T))\n",
    "    mask = torch.rand(N, T)\n",
    "    y[mask > p] = 0\n",
    "\n",
    "    # YOUR_TURN: Implement temporal_softmax_loss\n",
    "    print(temporal_softmax_loss(x, y, NULL_index).item())\n",
    "\n",
    "\n",
    "check_loss(1000, 1, 10, 1.0)  # Should be about 2.00-2.11\n",
    "check_loss(1000, 10, 10, 1.0)  # Should be about 20.6-21.0\n",
    "check_loss(5000, 10, 10, 0.1)  # Should be about 2.00-2.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWrmaSZaUxqX"
   },
   "source": [
    "## Captioning Module\n",
    "\n",
    "Now we are wrapping everything into the captioning module. Implement the `CaptioningRNN` module by following its instructions.\n",
    "\n",
    "skip the inference function (`CaptioningRNN.sample`) for now -- only implement `__init__` and `forward`.\n",
    "Run the following to check your forward pass using a small test case; you should see difference on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 664,
     "status": "ok",
     "timestamp": 1701244610385,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "d8a71FL_6bhZ",
    "outputId": "daf6de09-b07c-4c8b-8c3d-d7ea80c2c7ff"
   },
   "outputs": [],
   "source": [
    "from rnn_captioning import CaptioningRNN\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "N, D, W, H = 10, 400, 30, 40\n",
    "word_to_idx = {\"<NULL>\": 0, \"cat\": 2, \"dog\": 3}\n",
    "V = len(word_to_idx)\n",
    "T = 13\n",
    "\n",
    "model = CaptioningRNN(\n",
    "    word_to_idx,\n",
    "    input_dim=D,\n",
    "    wordvec_dim=W,\n",
    "    hidden_dim=H,\n",
    "    cell_type=\"rnn\",\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "# Copy parameters for sanity check:\n",
    "for k, v in model.named_parameters():\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).view(*v.shape))\n",
    "\n",
    "images = torch.randn(N, 3, *IMAGE_SHAPE)\n",
    "captions = (torch.arange(N * T) % V).view(N, T)\n",
    "\n",
    "loss = model(images, captions).item()\n",
    "expected_loss = 150.6090393066\n",
    "\n",
    "print(\"loss: \", loss)\n",
    "print(\"expected loss: \", expected_loss)\n",
    "print(\"difference: \", rel_error(torch.tensor(loss), torch.tensor(expected_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YAOcQ4h6bhc"
   },
   "source": [
    "## Overfit small data\n",
    "\n",
    "To make sure that everything is working as expected, we can try to overfit this image captioning model to a small subset of data.\n",
    "\n",
    "We have implemented the `train_captioner` function which accepts the model and training data, and runs a simple training loop - passing data to model, collecting training loss, then calling `backward()` to obtain gradients. These gradients are optimized using the [AdamW optimizer](https://arxiv.org/abs/1711.05101) (supported by PyTorch).\n",
    "You can read its implementation in `a5_helper.py`.\n",
    "\n",
    "We will overfit on a subset of 50 examples.\n",
    "You should see a final loss of less than `0.5` and it should be done fairly quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 11341,
     "status": "ok",
     "timestamp": 1701244656797,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "yzhsGRzk6bhd",
    "outputId": "7509e04a-b903-46a4-8381-477b7bc05ce5"
   },
   "outputs": [],
   "source": [
    "from a5_helper import train_captioner\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# data input\n",
    "small_num_train = 50\n",
    "sample_idx = torch.linspace(0, num_train - 1, steps=small_num_train).long()\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx]\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx]\n",
    "\n",
    "# optimization arguments\n",
    "num_epochs = 80\n",
    "\n",
    "# create the image captioning model\n",
    "model = CaptioningRNN(\n",
    "    cell_type=\"rnn\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "model = model.to(**to_float)\n",
    "\n",
    "for learning_rate in [1e-3]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    rnn_overfit, _ = train_captioner(\n",
    "        model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=OVR_BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiHsRysE6bhe"
   },
   "source": [
    "## Inference: Sampling Captions\n",
    "\n",
    "Unlike classification models, image captioning models behave very differently at training time and at test time.\n",
    "At training time, we have access to the ground-truth caption, so we feed ground-truth words as input to the RNN at each timestep.\n",
    "At test time, we sample from the distribution over the vocabulary at each timestep, and feed the sample as input to the RNN at the next timestep.\n",
    "\n",
    "Implement the `CaptioningRNN.sample` for test-time sampling. After doing so, run the following to train a captioning model and sample from the model on both training and validation data.\n",
    "\n",
    "### Train the image captioning model\n",
    "\n",
    "Now perform the training on the entire training set. You should see a final loss less than `2.0` and each epoch should take ~14s - 44s to run, depending on the GPU colab assigns you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 768075,
     "status": "ok",
     "timestamp": 1701245427332,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "dXHnPuM_FU7k",
    "outputId": "e4d1d78c-1186-40d8-bb2a-5eb8fd2c5f5d"
   },
   "outputs": [],
   "source": [
    "from a5_helper import train_captioner\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# data input\n",
    "small_num_train = num_train\n",
    "sample_idx = torch.randint(num_train, size=(small_num_train,))\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx]\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx]\n",
    "\n",
    "# create the image captioning model\n",
    "rnn_model = CaptioningRNN(\n",
    "    cell_type=\"rnn\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "\n",
    "for learning_rate in [1e-3]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    rnn_model_submit, rnn_loss_submit = train_captioner(\n",
    "        rnn_model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=60,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97xga3Q5GO8B"
   },
   "source": [
    "### Test-time sampling\n",
    "The samples on training data should be very good; the samples on validation data will probably make less sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2188,
     "status": "ok",
     "timestamp": 1701245429507,
     "user": {
      "displayName": "Behnaz Rivandi",
      "userId": "13624825503045834691"
     },
     "user_tz": -210
    },
    "id": "Rvt326nX6bhf",
    "outputId": "9e6eb326-cc75-41ac-cbbf-4e07e0214e25"
   },
   "outputs": [],
   "source": [
    "from a5_helper import decode_captions\n",
    "\n",
    "\n",
    "rnn_model.eval()\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    sample_idx = torch.randint(\n",
    "        0, num_train if split == \"train\" else num_val, (VIS_BATCH_SIZE,)\n",
    "    )\n",
    "    sample_images = data_dict[split + \"_images\"][sample_idx]\n",
    "    sample_captions = data_dict[split + \"_captions\"][sample_idx]\n",
    "\n",
    "    # decode_captions is loaded from a5_helper.py\n",
    "    gt_captions = decode_captions(sample_captions, data_dict[\"vocab\"][\"idx_to_token\"])\n",
    "\n",
    "    generated_captions = rnn_model.sample(sample_images.to(DEVICE))\n",
    "    generated_captions = decode_captions(\n",
    "        generated_captions, data_dict[\"vocab\"][\"idx_to_token\"]\n",
    "    )\n",
    "\n",
    "    for i in range(VIS_BATCH_SIZE):\n",
    "        plt.imshow(sample_images[i].permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"[{split}] RNN Generated: {generated_captions[i]}\\nGT: {gt_captions[i]}\"\n",
    "        )\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

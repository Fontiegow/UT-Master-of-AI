{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNaiveBayes:\n",
    "    \"\"\"\n",
    "    Custom implementation of the Naive Bayes classifier for text classification\n",
    "    using the Bag-of-Words model and Laplace Smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0):\n",
    "        # alpha is the Laplace smoothing factor (default to 1.0 for Add-One smoothing)\n",
    "        self.alpha = alpha\n",
    "        self.class_priors = {}        # Stores P(C) for each class\n",
    "        self.word_likelihoods = {}    # Stores P(W|C) for each word W and class C\n",
    "        self.vocab_size = 0           # Size of the vocabulary (V)\n",
    "\n",
    "    def fit(self, X_vec, y_labels, feature_names):\n",
    "        \"\"\"\n",
    "        Calculates the prior probabilities P(C) and the conditional likelihoods P(W|C).\n",
    "        \n",
    "        Parameters:\n",
    "        - X_vec: Document-Term Matrix (BoW), shape (n_samples, n_features)\n",
    "        - y_labels: Array of class labels (0 or 1)\n",
    "        - feature_names: List of words corresponding to the features (vocabulary)\n",
    "        \"\"\"\n",
    "        self.vocab_size = X_vec.shape[1]\n",
    "        self.feature_names = feature_names\n",
    "        total_samples = len(y_labels)\n",
    "        \n",
    "        unique_classes = np.unique(y_labels)\n",
    "        \n",
    "        # 1. Calculate Prior Probabilities P(C)\n",
    "        self.class_priors = {}\n",
    "        for c in unique_classes:\n",
    "            count = np.sum(y_labels == c)\n",
    "            self.class_priors[c] = count / total_samples\n",
    "        \n",
    "        # 2. Calculate Conditional Likelihoods P(W|C)\n",
    "        self.word_likelihoods = {}\n",
    "        for c in unique_classes:\n",
    "            # Filter data for the current class\n",
    "            X_c = X_vec[y_labels == c]\n",
    "            \n",
    "            # Sum of word counts for the current class (Token Count in Class C)\n",
    "            # This is the denominator's first part: Sum over all words W in C\n",
    "            total_tokens_in_c = np.sum(X_c)\n",
    "            \n",
    "            # Denominator: Sum of all tokens in C + (alpha * Vocab Size)\n",
    "            denominator = total_tokens_in_c + (self.alpha * self.vocab_size)\n",
    "            \n",
    "            # Dictionary to store P(W|C) for this class\n",
    "            self.word_likelihoods[c] = {}\n",
    "            \n",
    "            # Sum of counts for each word (Word Count W in C)\n",
    "            word_counts = np.sum(X_c, axis=0)\n",
    "            \n",
    "            # Calculate P(W|C) for every word in the vocabulary\n",
    "            for i, word in enumerate(self.feature_names):\n",
    "                # Numerator: Word Count + alpha (Laplace Smoothing)\n",
    "                numerator = word_counts[i] + self.alpha\n",
    "                \n",
    "                # P(W|C) = (Word Count + alpha) / (Total Tokens in C + alpha*V)\n",
    "                self.word_likelihoods[c][word] = numerator / denominator\n",
    "\n",
    "    def predict_one(self, sample_vector):\n",
    "        \"\"\"Classifies a single sample using log probabilities.\"\"\"\n",
    "        best_class = None\n",
    "        max_log_prob = -np.inf\n",
    "\n",
    "        for c, prior in self.class_priors.items():\n",
    "            # Start with the log of the prior probability: log(P(C))\n",
    "            current_log_prob = math.log(prior)\n",
    "            \n",
    "            # Iterate through the words (features) in the sample\n",
    "            for i, count in enumerate(sample_vector):\n",
    "                # We only need to consider words present in the sample (count > 0)\n",
    "                if count > 0:\n",
    "                    word = self.feature_names[i]\n",
    "                    \n",
    "                    # Get the likelihood P(W|C) for this word and class\n",
    "                    likelihood = self.word_likelihoods[c].get(word, 0)\n",
    "                    \n",
    "                    # Add the log likelihood for each occurrence of the word\n",
    "                    # We multiply the log(P(W|C)) by the count of the word, \n",
    "                    # based on the Multinomial Naive Bayes assumption.\n",
    "                    if likelihood > 0:\n",
    "                        current_log_prob += count * math.log(likelihood)\n",
    "            \n",
    "            # Check if this class is the best one so far\n",
    "            if current_log_prob > max_log_prob:\n",
    "                max_log_prob = current_log_prob\n",
    "                best_class = c\n",
    "                \n",
    "        return best_class\n",
    "\n",
    "    def predict(self, X_vec):\n",
    "        \"\"\"Predicts classes for an entire dataset.\"\"\"\n",
    "        predictions = [self.predict_one(sample) for sample in X_vec]\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part a*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Scenario 1: Minimal Preprocessing (Stop Words are kept)]\n",
      "Vocabulary Size: 128\n",
      "P('the' | Positive) (Stop Word): 0.050000\n",
      "P('wonderful' | Positive) (Sentiment Word): 0.006667\n",
      "\n",
      "[Scenario 2: Full Preprocessing (Stop Words are removed)]\n",
      "Vocabulary Size: 91\n",
      "P('wonderful' | Positive) (Sentiment Word): 0.010870\n",
      "\n",
      "--- Comparison: Effect of Stop Word Removal on P('wonderful' | Positive) ---\n",
      "P('wonderful' | Positive) in Scenario 1 (With Stop Words): 0.006667\n",
      "P('wonderful' | Positive) in Scenario 2 (Without Stop Words): 0.010870\n",
      "Ratio of increase: 1.63 times\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans raw text.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<br\\s*/>', ' ', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "STOP_WORDS = set([\n",
    "    'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'this', 'that', \n",
    "    'was', 'as', 'for', 'with', 'movie', 'film', 'but', 'on', 'are', \n",
    "    'not', 'have', 'be', 'one', 'all', 'at', 'by', 'an', 'who', 'so', \n",
    "    'from', 'like', 'there', 'or', 'just', 'about', 'out', 'if', 'has',\n",
    "    'what', 'some', 'good', 'can', 'more', 'when', 'very', 'up', 'no', \n",
    "    'time', 'my', 'even', 'would', 'she', 'which', 'only', 'really', \n",
    "    'see', 'story', 'their', 'had'\n",
    "])\n",
    "\n",
    "def tokenize_with_stopwords(text):\n",
    "    \"\"\"Minimal preprocessing (keeps stop words).\"\"\"\n",
    "    return clean_text(text).split(' ')\n",
    "\n",
    "def tokenize_without_stopwords(text):\n",
    "    \"\"\"Full preprocessing (removes stop words).\"\"\"\n",
    "    cleaned_tokens = clean_text(text).split(' ')\n",
    "    return [word for word in cleaned_tokens if word not in STOP_WORDS and len(word) > 1]\n",
    "\n",
    "class CustomCountVectorizer:\n",
    "    \"\"\"Bag-of-Words Vectorizer.\"\"\"\n",
    "    def __init__(self, preprocessor_func, max_features=None):\n",
    "        self.max_features = max_features\n",
    "        self.vocabulary = {}\n",
    "        self.feature_names = []\n",
    "        self.preprocessor = preprocessor_func\n",
    "        \n",
    "    def fit(self, raw_documents):\n",
    "        word_counts = defaultdict(int)\n",
    "        for doc in raw_documents:\n",
    "            tokens = self.preprocessor(doc)\n",
    "            for token in tokens:\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        sorted_words = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        if self.max_features:\n",
    "            sorted_words = sorted_words[:self.max_features]\n",
    "            \n",
    "        self.vocabulary = {word: idx for idx, (word, count) in enumerate(sorted_words)}\n",
    "        self.feature_names = [word for word, count in sorted_words]\n",
    "        \n",
    "    def transform(self, raw_documents):\n",
    "        n_samples = len(raw_documents)\n",
    "        n_features = len(self.vocabulary)\n",
    "        X = np.zeros((n_samples, n_features), dtype=int)\n",
    "        \n",
    "        for i, doc in enumerate(raw_documents):\n",
    "            tokens = self.preprocessor(doc)\n",
    "            for token in tokens:\n",
    "                if token in self.vocabulary:\n",
    "                    idx = self.vocabulary[token]\n",
    "                    X[i, idx] += 1\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, raw_documents):\n",
    "        self.fit(raw_documents)\n",
    "        return self.transform(raw_documents)\n",
    "\n",
    "class CustomNaiveBayes:\n",
    "    \"\"\"Naive Bayes classifier with Laplace Smoothing.\"\"\"\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_priors = {}\n",
    "        self.word_likelihoods = {}\n",
    "        self.vocab_size = 0\n",
    "        self.feature_names = []\n",
    "\n",
    "    def fit(self, X_vec, y_labels, feature_names):\n",
    "        self.vocab_size = X_vec.shape[1]\n",
    "        self.feature_names = feature_names\n",
    "        total_samples = len(y_labels)\n",
    "        unique_classes = np.unique(y_labels)\n",
    "        \n",
    "        for c in unique_classes:\n",
    "            count = np.sum(y_labels == c)\n",
    "            self.class_priors[c] = count / total_samples\n",
    "        \n",
    "        for c in unique_classes:\n",
    "            X_c = X_vec[y_labels == c]\n",
    "            total_tokens_in_c = np.sum(X_c)\n",
    "            denominator = total_tokens_in_c + (self.alpha * self.vocab_size)\n",
    "            \n",
    "            self.word_likelihoods[c] = {}\n",
    "            word_counts = np.sum(X_c, axis=0)\n",
    "            \n",
    "            for i, word in enumerate(self.feature_names):\n",
    "                numerator = word_counts[i] + self.alpha\n",
    "                self.word_likelihoods[c][word] = numerator / denominator\n",
    "\n",
    "    def predict(self, X_vec):\n",
    "        return np.zeros(len(X_vec))\n",
    "\n",
    "\n",
    "# --- Demonstration Setup (Mock Data) ---\n",
    "\n",
    "raw_reviews = [\n",
    "    \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.\", # Positive (1)\n",
    "    \"A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"\"has got all the polari\"\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece.\", # Positive (1)\n",
    "    \"The worst, most disappointing movie I have ever seen. Absolutely terrible and boring and a waste of money.\" # Negative (0)\n",
    "]\n",
    "raw_sentiments = np.array([1, 1, 0]) \n",
    "\n",
    "# --- Scenario 1: With Stop Words ---\n",
    "print(\"[Scenario 1: Minimal Preprocessing (Stop Words are kept)]\")\n",
    "vectorizer_min = CustomCountVectorizer(preprocessor_func=tokenize_with_stopwords)\n",
    "X_min = vectorizer_min.fit_transform(raw_reviews)\n",
    "model_min = CustomNaiveBayes(alpha=1.0)\n",
    "model_min.fit(X_min, raw_sentiments, vectorizer_min.feature_names)\n",
    "\n",
    "p_the_min = model_min.word_likelihoods[1].get('the', 0)\n",
    "p_wonderful_min = model_min.word_likelihoods[1].get('wonderful', 0)\n",
    "\n",
    "print(f\"Vocabulary Size: {len(vectorizer_min.feature_names)}\")\n",
    "print(f\"P('the' | Positive) (Stop Word): {p_the_min:.6f}\")\n",
    "print(f\"P('wonderful' | Positive) (Sentiment Word): {p_wonderful_min:.6f}\")\n",
    "\n",
    "\n",
    "# --- Scenario 2: Without Stop Words ---\n",
    "print(\"\\n[Scenario 2: Full Preprocessing (Stop Words are removed)]\")\n",
    "vectorizer_full = CustomCountVectorizer(preprocessor_func=tokenize_without_stopwords)\n",
    "X_full = vectorizer_full.fit_transform(raw_reviews)\n",
    "model_full = CustomNaiveBayes(alpha=1.0)\n",
    "model_full.fit(X_full, raw_sentiments, vectorizer_full.feature_names)\n",
    "\n",
    "p_wonderful_full = model_full.word_likelihoods[1].get('wonderful', 0)\n",
    "\n",
    "print(f\"Vocabulary Size: {len(vectorizer_full.feature_names)}\")\n",
    "print(f\"P('wonderful' | Positive) (Sentiment Word): {p_wonderful_full:.6f}\")\n",
    "\n",
    "\n",
    "# --- Comparison ---\n",
    "\n",
    "print(f\"\\n--- Comparison: Effect of Stop Word Removal on P('wonderful' | Positive) ---\")\n",
    "print(f\"P('wonderful' | Positive) in Scenario 1 (With Stop Words): {p_wonderful_min:.6f}\")\n",
    "print(f\"P('wonderful' | Positive) in Scenario 2 (Without Stop Words): {p_wonderful_full:.6f}\")\n",
    "print(f\"Ratio of increase: {p_wonderful_full / p_wonderful_min:.2f} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded real IMDB dataset.\n",
      "Data Split: Train=35000, Val=7500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 1. Helper Functions & Classes ---\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<br\\s*/>', ' ', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "STOP_WORDS = set([\n",
    "    'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'this', 'that', \n",
    "    'was', 'as', 'for', 'with', 'movie', 'film', 'but', 'on', 'are', \n",
    "    'not', 'have', 'be', 'one', 'all', 'at', 'by', 'an', 'who', 'so', \n",
    "    'from', 'like', 'there', 'or', 'just', 'about', 'out', 'if', 'has',\n",
    "    'what', 'some', 'good', 'can', 'more', 'when', 'very', 'up', 'no', \n",
    "    'time', 'my', 'even', 'would', 'she', 'which', 'only', 'really', \n",
    "    'see', 'story', 'their', 'had'\n",
    "])\n",
    "\n",
    "def tokenize_without_stopwords(text):\n",
    "    cleaned_tokens = clean_text(text).split(' ')\n",
    "    return [word for word in cleaned_tokens if word not in STOP_WORDS and len(word) > 1]\n",
    "\n",
    "class CustomCountVectorizer:\n",
    "    def __init__(self, preprocessor_func, max_features=None):\n",
    "        self.max_features = max_features\n",
    "        self.vocabulary = {}\n",
    "        self.feature_names = []\n",
    "        self.preprocessor = preprocessor_func\n",
    "        \n",
    "    def fit(self, raw_documents):\n",
    "        word_counts = defaultdict(int)\n",
    "        for doc in raw_documents:\n",
    "            tokens = self.preprocessor(doc)\n",
    "            for token in tokens:\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        sorted_words = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "        if self.max_features:\n",
    "            sorted_words = sorted_words[:self.max_features]\n",
    "        self.vocabulary = {word: idx for idx, (word, count) in enumerate(sorted_words)}\n",
    "        self.feature_names = [word for word, count in sorted_words]\n",
    "        \n",
    "    def transform(self, raw_documents):\n",
    "        n_samples = len(raw_documents)\n",
    "        n_features = len(self.vocabulary)\n",
    "        X = np.zeros((n_samples, n_features), dtype=int)\n",
    "        for i, doc in enumerate(raw_documents):\n",
    "            tokens = self.preprocessor(doc)\n",
    "            for token in tokens:\n",
    "                if token in self.vocabulary:\n",
    "                    idx = self.vocabulary[token]\n",
    "                    X[i, idx] += 1\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, raw_documents):\n",
    "        self.fit(raw_documents)\n",
    "        return self.transform(raw_documents)\n",
    "\n",
    "class CustomNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_priors = {}\n",
    "        self.word_likelihoods = {}\n",
    "        self.vocab_size = 0\n",
    "        self.feature_names = []\n",
    "\n",
    "    def fit(self, X_vec, y_labels, feature_names):\n",
    "        self.vocab_size = X_vec.shape[1]\n",
    "        self.feature_names = feature_names\n",
    "        total_samples = len(y_labels)\n",
    "        unique_classes = np.unique(y_labels)\n",
    "        \n",
    "        for c in unique_classes:\n",
    "            self.class_priors[c] = np.sum(y_labels == c) / total_samples\n",
    "        \n",
    "        for c in unique_classes:\n",
    "            X_c = X_vec[y_labels == c]\n",
    "            total_tokens_in_c = np.sum(X_c)\n",
    "            denominator = total_tokens_in_c + (self.alpha * self.vocab_size)\n",
    "            self.word_likelihoods[c] = {}\n",
    "            word_counts = np.sum(X_c, axis=0)\n",
    "            for i, word in enumerate(self.feature_names):\n",
    "                numerator = word_counts[i] + self.alpha\n",
    "                self.word_likelihoods[c][word] = numerator / denominator\n",
    "\n",
    "    def predict_one(self, sample_vector):\n",
    "        best_class = None\n",
    "        max_log_prob = -np.inf\n",
    "        for c, prior in self.class_priors.items():\n",
    "            current_log_prob = math.log(prior)\n",
    "            for i, count in enumerate(sample_vector):\n",
    "                if count > 0:\n",
    "                    word = self.feature_names[i]\n",
    "                    likelihood = self.word_likelihoods[c].get(word, 0)\n",
    "                    if likelihood > 0:\n",
    "                        current_log_prob += count * math.log(likelihood)\n",
    "            if current_log_prob > max_log_prob:\n",
    "                max_log_prob = current_log_prob\n",
    "                best_class = c\n",
    "        return best_class\n",
    "\n",
    "    def predict(self, X_vec):\n",
    "        return np.array([self.predict_one(sample) for sample in X_vec])\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return {'F1-Score': f1_score}\n",
    "\n",
    "# --- 2. Data Preparation ---\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def manual_stratified_split(X, y, test_size, random_state, stratify):\n",
    "    np.random.seed(random_state)\n",
    "    X_split, y_split = [], []\n",
    "    unique_classes = np.unique(stratify)\n",
    "    for c in unique_classes:\n",
    "        class_indices = np.where(stratify == c)[0]\n",
    "        X_c = X[class_indices]\n",
    "        y_c = y[class_indices]\n",
    "        n_c = len(X_c)\n",
    "        shuffled_indices = np.random.permutation(n_c)\n",
    "        X_c = X_c[shuffled_indices]\n",
    "        y_c = y_c[shuffled_indices]\n",
    "        split_point = int(n_c * (1 - test_size))\n",
    "        X_split.append((X_c[:split_point], X_c[split_point:]))\n",
    "        y_split.append((y_c[:split_point], y_c[split_point:]))\n",
    "    \n",
    "    X_train_final = np.concatenate([item[0] for item in X_split])\n",
    "    X_test_final = np.concatenate([item[1] for item in X_split])\n",
    "    y_train_final = np.concatenate([item[0] for item in y_split])\n",
    "    y_test_final = np.concatenate([item[1] for item in y_split])\n",
    "    \n",
    "    # Shuffle finally\n",
    "    train_indices = np.random.permutation(len(X_train_final))\n",
    "    return X_train_final[train_indices], X_test_final, y_train_final[train_indices], y_test_final\n",
    "\n",
    "# Try loading real data or fallback to placeholder\n",
    "try:\n",
    "    df = pd.read_csv('IMDB.csv')\n",
    "    print(\"âœ… Loaded real IMDB dataset.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ðŸš¨ Real file not found. Using PLACEHOLDER data for demo.\")\n",
    "    n_samples = 500\n",
    "    texts = [f\"review_{i} wonderful story\" if i < n_samples/2 else f\"review_{i} terrible movie\" for i in range(n_samples)]\n",
    "    sentiments = [\"positive\"] * (n_samples // 2) + [\"negative\"] * (n_samples // 2)\n",
    "    df = pd.DataFrame({'review': texts, 'sentiment': sentiments})\n",
    "\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "X = df['review'].values\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# Split 70% Train, 30% Temp\n",
    "X_train_raw, X_temp, y_train, y_temp = manual_stratified_split(X, y, test_size=0.3, random_state=SEED, stratify=y)\n",
    "# Split Temp 50/50 -> 15% Val, 15% Test\n",
    "X_val_raw, X_test_raw, y_val, y_test = manual_stratified_split(X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp)\n",
    "\n",
    "# Assign to variables expected in Block 2\n",
    "X_train = X_train_raw\n",
    "X_val = X_val_raw\n",
    "\n",
    "print(f\"Data Split: Train={len(X_train)}, Val={len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Experiment: F1 vs Max Features ---\n",
      "Max Features: 10 | Vocab Size: 10 | F1-Score: 0.5310\n",
      "Max Features: 50 | Vocab Size: 50 | F1-Score: 0.6785\n",
      "Max Features: 100 | Vocab Size: 100 | F1-Score: 0.7024\n",
      "Max Features: 200 | Vocab Size: 200 | F1-Score: 0.7474\n",
      "\n",
      "Final F1 Scores: [np.float64(0.5310066339775023), np.float64(0.6785036595283274), np.float64(0.7023583626999187), np.float64(0.747412286597661)]\n"
     ]
    }
   ],
   "source": [
    "# --- Start Part (b): F1-Score vs. Max Features Search ---\n",
    "\n",
    "# If using real data, uncomment the line below:\n",
    "# feature_counts = [100, 500, 1000, 5000, 10000, 20000]\n",
    "\n",
    "# For placeholder/demo data, we use smaller counts:\n",
    "feature_counts = [10, 50, 100, 200] \n",
    "\n",
    "results_f1 = []\n",
    "alpha_fixed = 1.0 \n",
    "\n",
    "print(f\"\\n--- Running Experiment: F1 vs Max Features ---\")\n",
    "\n",
    "for max_feat in feature_counts:\n",
    "    # 1. Vectorize\n",
    "    vectorizer = CustomCountVectorizer(preprocessor_func=tokenize_without_stopwords, max_features=max_feat)\n",
    "    X_train_vec = vectorizer.fit_transform(X_train) \n",
    "    X_val_vec = vectorizer.transform(X_val)\n",
    "    \n",
    "    # 2. Train\n",
    "    model = CustomNaiveBayes(alpha=alpha_fixed)\n",
    "    model.fit(X_train_vec, y_train, vectorizer.feature_names)\n",
    "    \n",
    "    # 3. Evaluate\n",
    "    y_val_pred = model.predict(X_val_vec)\n",
    "    metrics = calculate_metrics(y_val, y_val_pred) \n",
    "    \n",
    "    results_f1.append(metrics['F1-Score'])\n",
    "    \n",
    "    print(f\"Max Features: {max_feat} | Vocab Size: {len(vectorizer.feature_names)} | F1-Score: {metrics['F1-Score']:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal F1 Scores: {results_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Experiment: Large Feature Counts ---\n",
      "Max Features: 1000 | F1-Score: 0.8270\n",
      "Max Features: 5000 | F1-Score: 0.8449\n",
      "Max Features: 10000 | F1-Score: 0.8466\n",
      "Max Features: 20000 | F1-Score: 0.8490\n",
      "Max Features: 30000 | F1-Score: 0.8506\n"
     ]
    }
   ],
   "source": [
    "# --- Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø®Ø´ (Ø¨): Ø¨Ø±Ø±Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± ---\n",
    "\n",
    "# Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ø²Ø±Ú¯ØªØ± Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¯Ù† Ø§Ø«Ø± ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù†Ù‚Ø·Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡\n",
    "feature_counts_large = [1000, 5000, 10000, 20000, 30000] \n",
    "\n",
    "print(f\"\\n--- Running Experiment: Large Feature Counts ---\")\n",
    "\n",
    "for max_feat in feature_counts_large:\n",
    "    # 1. Vectorize\n",
    "    vectorizer = CustomCountVectorizer(preprocessor_func=tokenize_without_stopwords, max_features=max_feat)\n",
    "    X_train_vec = vectorizer.fit_transform(X_train) \n",
    "    X_val_vec = vectorizer.transform(X_val)\n",
    "    \n",
    "    # 2. Train\n",
    "    model = CustomNaiveBayes(alpha=1.0)\n",
    "    model.fit(X_train_vec, y_train, vectorizer.feature_names)\n",
    "    \n",
    "    # 3. Evaluate\n",
    "    y_val_pred = model.predict(X_val_vec)\n",
    "    metrics = calculate_metrics(y_val, y_val_pred) \n",
    "    \n",
    "    print(f\"Max Features: {max_feat} | F1-Score: {metrics['F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part c**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Experiment: F1 vs Alpha (fixed max_features=5000) ---\n",
      "Alpha: 0.0001 | F1-Score: 0.8457\n",
      "Alpha: 0.01 | F1-Score: 0.8457\n",
      "Alpha: 0.1 | F1-Score: 0.8458\n",
      "Alpha: 1.0 | F1-Score: 0.8449\n",
      "Alpha: 5.0 | F1-Score: 0.8443\n",
      "Alpha: 10.0 | F1-Score: 0.8446\n",
      "Alpha: 50.0 | F1-Score: 0.8410\n",
      "Alpha: 100.0 | F1-Score: 0.8368\n",
      "\n",
      "Final F1 Scores for Alphas: [np.float64(0.8456716019741229), np.float64(0.8456716019741229), np.float64(0.8457844183564568), np.float64(0.8449426207632772), np.float64(0.844284188034188), np.float64(0.8446407065435568), np.float64(0.841040852096535), np.float64(0.8367734926670288)]\n"
     ]
    }
   ],
   "source": [
    "# --- Start Part (c): F1-Score vs. Alpha (Laplace Smoothing) ---\n",
    "\n",
    "# 1. Fix Max Features to a reasonable optimal value found in Part (b)\n",
    "optimal_max_features = 5000 \n",
    "vectorizer = CustomCountVectorizer(preprocessor_func=tokenize_without_stopwords, max_features=optimal_max_features)\n",
    "\n",
    "# Vectorize once (since features are fixed)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "\n",
    "# 2. Define range of Alpha values\n",
    "# We test extremely small values, the default (1.0), and very large values\n",
    "alpha_values = [0.0001, 0.01, 0.1, 1.0, 5.0, 10.0, 50.0, 100.0]\n",
    "results_alpha = []\n",
    "\n",
    "print(f\"\\n--- Running Experiment: F1 vs Alpha (fixed max_features={optimal_max_features}) ---\")\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    # Train model with current alpha\n",
    "    model = CustomNaiveBayes(alpha=alpha)\n",
    "    model.fit(X_train_vec, y_train, vectorizer.feature_names)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_val_pred = model.predict(X_val_vec)\n",
    "    metrics = calculate_metrics(y_val, y_val_pred)\n",
    "    \n",
    "    results_alpha.append(metrics['F1-Score'])\n",
    "    print(f\"Alpha: {alpha} | F1-Score: {metrics['F1-Score']:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal F1 Scores for Alphas: {results_alpha}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

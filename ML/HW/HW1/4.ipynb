{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb08670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with 10000 samples.\n",
      "Data Split: Train=3500, Validation=750, Test=750\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# --- 1. Data Preparation and Splitting ---\n",
    "\n",
    "# The column names provided by the user\n",
    "COLUMNS = ['asia', 'tub', 'smoke', 'bronc', 'either', 'xray', 'dysp', 'lung']\n",
    "SEED = 42\n",
    "\n",
    "def load_and_prepare_data(csv_path=None, simulate_size=5000):\n",
    "    \"\"\"\n",
    "    Loads or simulates the ASIA dataset and converts categorical data to numerical (0/1).\n",
    "    \"\"\"\n",
    "    if csv_path:\n",
    "        # Attempt to load the user's data (assuming it's a CSV or similar format)\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            if df.shape[0] < 500: # Check if data is too small for structure learning\n",
    "                print(f\"Warning: Loaded data size ({df.shape[0]}) is too small. Simulating a larger set.\")\n",
    "                # Fallback to simulation for meaningful structure learning\n",
    "                return simulate_asia_data(simulate_size)\n",
    "            print(f\"Loaded data with {df.shape[0]} samples.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CSV not found. Simulating ASIA dataset...\")\n",
    "            return simulate_asia_data(simulate_size)\n",
    "    \n",
    "    # Simple simulation logic (for demonstrative purposes, based on standard ASIA net)\n",
    "    return simulate_asia_data(simulate_size)\n",
    "\n",
    "def simulate_asia_data(n_samples):\n",
    "    \"\"\"\n",
    "    Simulates a large dataset based on the known structure of the ASIA network.\n",
    "    (Used to ensure K2 has enough data to find meaningful dependencies)\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    data = {}\n",
    "    \n",
    "    # 8 variables: A, T, S, L, B, E, X, D (using user's names: asia, tub, smoke, lung, bronc, either, xray, dysp)\n",
    "    # A: asia, T: tub, S: smoke, L: lung, B: bronc, E: either, X: xray, D: dysp\n",
    "    \n",
    "    data['smoke'] = np.random.randint(0, 2, size=n_samples) # S\n",
    "    data['asia'] = np.random.randint(0, 2, size=n_samples) # A\n",
    "    \n",
    "    # T = f(A) -> Tub depends on Asia\n",
    "    data['tub'] = np.where(data['asia'] == 1, \n",
    "                           np.random.binomial(1, p=0.05, size=n_samples),  # P(T|A) = 0.05\n",
    "                           np.random.binomial(1, p=0.01, size=n_samples)) # P(T|not A) = 0.01\n",
    "\n",
    "    # L = f(S) -> Lung depends on Smoke\n",
    "    data['lung'] = np.where(data['smoke'] == 1,\n",
    "                            np.random.binomial(1, p=0.05, size=n_samples),\n",
    "                            np.random.binomial(1, p=0.001, size=n_samples))\n",
    "    \n",
    "    # B = f(S) -> Bronc depends on Smoke\n",
    "    data['bronc'] = np.where(data['smoke'] == 1,\n",
    "                            np.random.binomial(1, p=0.6, size=n_samples),\n",
    "                            np.random.binomial(1, p=0.3, size=n_samples))\n",
    "    \n",
    "    # E = f(T, L) -> Either depends on Tub and Lung\n",
    "    P_E = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        if data['tub'][i] == 1 or data['lung'][i] == 1:\n",
    "            P_E[i] = 0.99\n",
    "        else:\n",
    "            P_E[i] = 0.01\n",
    "    data['either'] = np.where(np.random.rand(n_samples) < P_E, 1, 0)\n",
    "\n",
    "    # X = f(E) -> Xray depends on Either\n",
    "    data['xray'] = np.where(data['either'] == 1,\n",
    "                            np.random.binomial(1, p=0.98, size=n_samples),\n",
    "                            np.random.binomial(1, p=0.05, size=n_samples))\n",
    "\n",
    "    # D = f(E, B) -> Dysp depends on Either and Bronc\n",
    "    P_D = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        if data['either'][i] == 1 and data['bronc'][i] == 1:\n",
    "            P_D[i] = 0.95\n",
    "        elif data['either'][i] == 1 and data['bronc'][i] == 0:\n",
    "            P_D[i] = 0.85\n",
    "        elif data['either'][i] == 0 and data['bronc'][i] == 1:\n",
    "            P_D[i] = 0.70\n",
    "        else:\n",
    "            P_D[i] = 0.10\n",
    "    data['dysp'] = np.where(np.random.rand(n_samples) < P_D, 1, 0)\n",
    "    \n",
    "    return pd.DataFrame(data)[COLUMNS]\n",
    "\n",
    "\n",
    "def stratified_split(df, target_col=None, test_size=0.3, random_state=SEED):\n",
    "    \"\"\"\n",
    "    Performs a simple split (not strictly stratified if target is None, but useful).\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Simple shuffle and split\n",
    "    df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    train_end = int(df.shape[0] * (1 - test_size))\n",
    "    val_end = int(df.shape[0] * (1 - test_size / 2))\n",
    "\n",
    "    train_df = df.iloc[:train_end]\n",
    "    val_df = df.iloc[train_end:val_end]\n",
    "    test_df = df.iloc[val_end:]\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Load and split data\n",
    "df_asia = load_and_prepare_data(csv_path='ASIA.csv') # Attempt to load user file first\n",
    "df_train, df_val, df_test = stratified_split(df_asia, test_size=0.3)\n",
    "\n",
    "print(f\"Data Split: Train={len(df_train)}, Validation={len(df_val)}, Test={len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eea8eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Bayesian Network Class and CPT Learning ---\n",
    "\n",
    "class BayesianNetwork:\n",
    "    def __init__(self, nodes):\n",
    "        self.nodes = nodes # List of node names\n",
    "        self.structure = {node: [] for node in nodes} # Adjacency list: node -> list of parents\n",
    "        self.cpts = {} # Conditional Probability Tables: node -> dict of CPTs\n",
    "\n",
    "    def learn_cpts(self, data):\n",
    "        \"\"\"Calculates CPTs for the current structure using Maximum Likelihood Estimation (MLE).\"\"\"\n",
    "        self.cpts = {}\n",
    "        for node in self.nodes:\n",
    "            parents = self.structure[node]\n",
    "            \n",
    "            # Group data by parent configurations\n",
    "            if parents:\n",
    "                grouped = data.groupby(parents)\n",
    "                counts = grouped[node].agg(['count', 'sum'])\n",
    "                \n",
    "                # Calculate P(Node=1 | Parents)\n",
    "                # Note: 'sum' is count of Node=1, 'count' is total observations\n",
    "                cpt_1 = (counts['sum'] + 1) / (counts['count'] + 2) # Laplace Smoothing (alpha=1)\n",
    "                cpt_0 = 1.0 - cpt_1\n",
    "                \n",
    "                self.cpts[node] = {\n",
    "                    'P_1': cpt_1.fillna(0.5), # Handle missing parent combinations\n",
    "                    'P_0': cpt_0.fillna(0.5) \n",
    "                }\n",
    "            else:\n",
    "                # Prior probability P(Node)\n",
    "                count_1 = data[node].sum()\n",
    "                total_count = len(data)\n",
    "                \n",
    "                # Laplace Smoothing\n",
    "                prob_1 = (count_1 + 1) / (total_count + 2)\n",
    "                self.cpts[node] = {\n",
    "                    'P_1': prob_1,\n",
    "                    'P_0': 1.0 - prob_1\n",
    "                }\n",
    "\n",
    "    def score_local_log_likelihood(self, node, parents, data):\n",
    "        \"\"\"\n",
    "        Calculates the local Log-Likelihood score for a node given its parents.\n",
    "        The K2 algorithm uses this to find the best parent set.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Count the occurrences (N_ijk) for the score calculation\n",
    "        if parents:\n",
    "            # Group by parents and the node itself\n",
    "            group_cols = parents + [node]\n",
    "            counts_df = data.groupby(group_cols).size().reset_index(name='N_ijk')\n",
    "            \n",
    "            # Group by parents only (N_ij)\n",
    "            parent_counts = data.groupby(parents).size().reset_index(name='N_ij')\n",
    "            parent_counts = parent_counts.set_index(parents)\n",
    "            \n",
    "            score = 0\n",
    "            for index, row in counts_df.iterrows():\n",
    "                # Node states (k=0, 1), Parent config states (j)\n",
    "                n_ijk = row['N_ijk']\n",
    "                parent_config = tuple(row[p] for p in parents)\n",
    "                \n",
    "                # N_ij = total count for this parent configuration\n",
    "                n_ij = parent_counts.loc[parent_config]['N_ij']\n",
    "                \n",
    "                # K2 Score (approx. Log-Likelihood with Dirichlet prior)\n",
    "                # log(N_ijk!) - log(N_ij + r_i)! where r_i=2 (binary node)\n",
    "                # Simplified form based on counts: log(P(X_i|Pa_i))\n",
    "                \n",
    "                # The K2 score formula often used is a simpler log-likelihood\n",
    "                # which is log(P(Node=k|Parents=j))\n",
    "                # For MLE: log(N_ijk / N_ij)\n",
    "                \n",
    "                if n_ijk > 0 and n_ij > 0:\n",
    "                     # This is the Log-Likelihood term: N_ijk * log(N_ijk / N_ij)\n",
    "                    score += n_ijk * math.log(n_ijk / n_ij)\n",
    "\n",
    "            return score\n",
    "        else:\n",
    "            # No parents (prior probability)\n",
    "            counts = data[node].value_counts().sort_index()\n",
    "            n_1 = counts.get(1, 0)\n",
    "            n_0 = counts.get(0, 0)\n",
    "            N = n_1 + n_0\n",
    "            \n",
    "            score = 0\n",
    "            if n_1 > 0:\n",
    "                score += n_1 * math.log(n_1 / N)\n",
    "            if n_0 > 0:\n",
    "                score += n_0 * math.log(n_0 / N)\n",
    "                \n",
    "            return score\n",
    "\n",
    "# --- 3. K2 Algorithm Implementation ---\n",
    "\n",
    "def k2_algorithm(data, node_ordering, max_parents=1):\n",
    "    \"\"\"\n",
    "    Implements the K2 structure learning algorithm.\n",
    "    \"\"\"\n",
    "    nodes = list(data.columns)\n",
    "    bn = BayesianNetwork(nodes)\n",
    "    \n",
    "    # K2 score is based on the training data\n",
    "    df_train_k2 = data.copy()\n",
    "    \n",
    "    for i, node in enumerate(node_ordering):\n",
    "        # 1. Potential parents: all nodes that come BEFORE the current node in the ordering\n",
    "        potential_parents = node_ordering[:i]\n",
    "        \n",
    "        best_parents = []\n",
    "        best_score = bn.score_local_log_likelihood(node, [], df_train_k2) # Start with no parents\n",
    "        \n",
    "        # 2. Greedy search for parents\n",
    "        while True:\n",
    "            candidate_parent = None\n",
    "            max_gain = -float('inf')\n",
    "            \n",
    "            # Check parents not yet in the best set, and within max_parents limit\n",
    "            P_current = set(best_parents)\n",
    "            candidates = [p for p in potential_parents if p not in P_current]\n",
    "            \n",
    "            # Stop if max parents reached\n",
    "            if len(best_parents) >= max_parents:\n",
    "                break\n",
    "                \n",
    "            # Iterate through all candidate parents\n",
    "            for p_candidate in candidates:\n",
    "                P_new = best_parents + [p_candidate]\n",
    "                \n",
    "                # Calculate new score\n",
    "                new_score = bn.score_local_log_likelihood(node, P_new, df_train_k2)\n",
    "                gain = new_score - best_score\n",
    "                \n",
    "                if gain > max_gain:\n",
    "                    max_gain = gain\n",
    "                    candidate_parent = p_candidate\n",
    "            \n",
    "            # Decision rule: only add if the gain is positive (or greater than a small tolerance)\n",
    "            if max_gain > 1e-6: \n",
    "                best_parents.append(candidate_parent)\n",
    "                best_score = best_score + max_gain # Update score efficiently\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # 3. Finalize structure for the current node\n",
    "        bn.structure[node] = best_parents\n",
    "        \n",
    "    # 4. Learn CPTs based on the final structure\n",
    "    bn.learn_cpts(df_train_k2)\n",
    "    \n",
    "    return bn\n",
    "\n",
    "# --- 4. Evaluation Function ---\n",
    "\n",
    "def calculate_average_log_likelihood(bn, data):\n",
    "    \"\"\"\n",
    "    Calculates the average Log-Likelihood of the data given the network structure and CPTs.\n",
    "    P(D|G) = sum_i log( P(X_i|Pa_i) ) for each sample\n",
    "    \"\"\"\n",
    "    log_likelihoods = []\n",
    "    \n",
    "    for _, sample in data.iterrows():\n",
    "        total_log_prob = 0\n",
    "        for node in bn.nodes:\n",
    "            # 1. Get CPT\n",
    "            cpts = bn.cpts[node]\n",
    "            node_value = sample[node]\n",
    "            \n",
    "            # 2. Determine the parent configuration\n",
    "            parents = bn.structure[node]\n",
    "            \n",
    "            if parents:\n",
    "                parent_config_values = tuple(sample[p] for p in parents)\n",
    "                \n",
    "                if node_value == 1:\n",
    "                    # Get P(Node=1 | Parents=config)\n",
    "                    # Need to safely access the CPT (index by parent config)\n",
    "                    prob = cpts['P_1'].get(parent_config_values) if isinstance(cpts['P_1'], pd.Series) else cpts['P_1'].get(parent_config_values)\n",
    "                else:\n",
    "                    # Get P(Node=0 | Parents=config)\n",
    "                    prob = cpts['P_0'].get(parent_config_values) if isinstance(cpts['P_0'], pd.Series) else cpts['P_0'].get(parent_config_values)\n",
    "                    \n",
    "                # Handle cases where the parent configuration was not seen in training\n",
    "                if prob is None:\n",
    "                    # Fallback to 0.5 (or prior knowledge, here we use a small default)\n",
    "                    prob = 1e-6 # Arbitrarily small probability to avoid log(0)\n",
    "            else:\n",
    "                # No parents: use prior P(Node)\n",
    "                prob = cpts['P_1'] if node_value == 1 else cpts['P_0']\n",
    "            \n",
    "            # 3. Add log probability to total\n",
    "            total_log_prob += math.log(prob)\n",
    "        \n",
    "        log_likelihoods.append(total_log_prob)\n",
    "        \n",
    "    return np.mean(log_likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf994018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running K2 Algorithm Structure Learning ---\n",
      "\n",
      "Processing Order_1: ['asia', 'tub', 'smoke', 'bronc', 'either', 'xray', 'dysp', 'lung']\n",
      "  Max Parents: 1 -> Avg Log-Likelihood: -97.4016\n",
      "  Max Parents: 2 -> Avg Log-Likelihood: -16.5646\n",
      "  Max Parents: 3 -> Avg Log-Likelihood: -16.5659\n",
      "  Max Parents: 4 -> Avg Log-Likelihood: -16.5697\n",
      "\n",
      "Processing Order_2: ['lung', 'dysp', 'xray', 'either', 'bronc', 'smoke', 'tub', 'asia']\n",
      "  Max Parents: 1 -> Avg Log-Likelihood: -96.8219\n",
      "  Max Parents: 2 -> Avg Log-Likelihood: -16.0438\n",
      "  Max Parents: 3 -> Avg Log-Likelihood: -16.0497\n",
      "  Max Parents: 4 -> Avg Log-Likelihood: -16.0533\n",
      "\n",
      "Processing Order_3: ['smoke', 'asia', 'lung', 'tub', 'bronc', 'either', 'dysp', 'xray']\n",
      "  Max Parents: 1 -> Avg Log-Likelihood: -97.4017\n",
      "  Max Parents: 2 -> Avg Log-Likelihood: -15.9699\n",
      "  Max Parents: 3 -> Avg Log-Likelihood: -15.9935\n",
      "  Max Parents: 4 -> Avg Log-Likelihood: -15.9974\n",
      "\n",
      "Processing Order_4: ['xray', 'dysp', 'asia', 'smoke', 'tub', 'lung', 'either', 'bronc']\n",
      "  Max Parents: 1 -> Avg Log-Likelihood: -97.0002\n",
      "  Max Parents: 2 -> Avg Log-Likelihood: -16.0964\n",
      "  Max Parents: 3 -> Avg Log-Likelihood: -16.0344\n",
      "  Max Parents: 4 -> Avg Log-Likelihood: -16.0267\n",
      "\n",
      "Processing Order_5: ['tub', 'either', 'lung', 'dysp', 'smoke', 'asia', 'bronc', 'xray']\n",
      "  Max Parents: 1 -> Avg Log-Likelihood: -96.8171\n",
      "  Max Parents: 2 -> Avg Log-Likelihood: -16.5618\n",
      "  Max Parents: 3 -> Avg Log-Likelihood: -16.5623\n",
      "  Max Parents: 4 -> Avg Log-Likelihood: -16.5669\n",
      "\n",
      "--- Summary of K2 Results (Validation Log-Likelihood) ---\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\venv\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1206\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1178\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1142\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tabulate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m max_p \u001b[38;5;129;01min\u001b[39;00m max_parent_limits:\n\u001b[32m     62\u001b[39m     df_scores[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMax P=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_p\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] = results_table[max_p].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_scores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# The image tag for the plot goes here\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:2994\u001b[39m, in \u001b[36mDataFrame.to_markdown\u001b[39m\u001b[34m(self, buf, mode, index, storage_options, **kwargs)\u001b[39m\n\u001b[32m   2992\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mtablefmt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2993\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mshowindex\u001b[39m\u001b[33m\"\u001b[39m, index)\n\u001b[32m-> \u001b[39m\u001b[32m2994\u001b[39m tabulate = \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtabulate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2995\u001b[39m result = tabulate.tabulate(\u001b[38;5;28mself\u001b[39m, **kwargs)\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\venv\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
     ]
    }
   ],
   "source": [
    "# --- Experiment Setup (Part a) ---\n",
    "\n",
    "# 1. Define 5 different node orderings\n",
    "node_orderings = [\n",
    "    COLUMNS, # 1. Original/Sequential Order\n",
    "    ['lung', 'dysp', 'xray', 'either', 'bronc', 'smoke', 'tub', 'asia'], # 2. Reverse Order\n",
    "    ['smoke', 'asia', 'lung', 'tub', 'bronc', 'either', 'dysp', 'xray'], # 3. Causal-like order (A/S first, effects later)\n",
    "    ['xray', 'dysp', 'asia', 'smoke', 'tub', 'lung', 'either', 'bronc'], # 4. Mixed/Random\n",
    "    ['tub', 'either', 'lung', 'dysp', 'smoke', 'asia', 'bronc', 'xray']  # 5. Another Random\n",
    "]\n",
    "\n",
    "max_parent_limits = [1, 2, 3, 4]\n",
    "results = {}\n",
    "\n",
    "print(\"\\n--- Running K2 Algorithm Structure Learning ---\")\n",
    "\n",
    "for i, ordering in enumerate(node_orderings):\n",
    "    order_name = f\"Order_{i+1}\"\n",
    "    results[order_name] = {}\n",
    "    \n",
    "    print(f\"\\nProcessing {order_name}: {ordering}\")\n",
    "    \n",
    "    for max_p in max_parent_limits:\n",
    "        print(f\"  Max Parents: {max_p}\", end=' -> ')\n",
    "        \n",
    "        # 1. Learn Structure\n",
    "        bn_model = k2_algorithm(df_train, ordering, max_parents=max_p)\n",
    "        \n",
    "        # 2. Evaluate on Validation Data\n",
    "        avg_log_likelihood = calculate_average_log_likelihood(bn_model, df_val)\n",
    "        \n",
    "        results[order_name][max_p] = {\n",
    "            'score': avg_log_likelihood,\n",
    "            'edges': sum(len(parents) for parents in bn_model.structure.values()),\n",
    "            'model': bn_model\n",
    "        }\n",
    "        print(f\"Avg Log-Likelihood: {avg_log_likelihood:.4f}\")\n",
    "\n",
    "# 3. Find Best and Most Edges Networks\n",
    "best_score = -float('inf')\n",
    "best_bn = None\n",
    "most_edges = 0\n",
    "most_edges_bn = None\n",
    "\n",
    "for order in results:\n",
    "    for max_p in results[order]:\n",
    "        current = results[order][max_p]\n",
    "        if current['score'] > best_score:\n",
    "            best_score = current['score']\n",
    "            best_bn = current['model']\n",
    "        \n",
    "        if current['edges'] > most_edges:\n",
    "            most_edges = current['edges']\n",
    "            most_edges_bn = current['model']\n",
    "\n",
    "# 4. Print Results Table\n",
    "\n",
    "print(\"\\n--- Summary of K2 Results (Validation Log-Likelihood) ---\")\n",
    "results_table = pd.DataFrame(results).T\n",
    "df_scores = pd.DataFrame()\n",
    "for max_p in max_parent_limits:\n",
    "    df_scores[f'Max P={max_p}'] = results_table[max_p].apply(lambda x: x['score'])\n",
    "    \n",
    "print(df_scores.to_markdown())\n",
    "\n",
    "# The image tag for the plot goes here\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebc6d0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Classification on Test Data (Target: Lung) ---\n",
      "K2 Optimal (Max P=2): Acc=0.9933, F1=0.8837\n",
      "True Structure (ASIA): Acc=0.9933, F1=0.8837\n",
      "Naive Bayes (Lung Target): Acc=0.9693, F1=0.6102\n",
      "\n",
      "--- Classification Results Summary (Test Data) ---\n",
      "Model                     | Accuracy   | Precision  | Recall     | F1-Score  \n",
      "---------------------------------------------------------------------------\n",
      "K2 Optimal (Max P=2)      | 0.9933     | 0.8636     | 0.9048     | 0.8837    \n",
      "True Structure (ASIA)     | 0.9933     | 0.8636     | 0.9048     | 0.8837    \n",
      "Naive Bayes (Lung Target) | 0.9693     | 0.4737     | 0.8571     | 0.6102    \n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define calculate_metrics (Ensuring it's available) ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Assuming the classes BayesianNetwork, k2_algorithm, predict_classification, and dataframes \n",
    "# (df_train, df_val, df_test) are already defined and in memory from previous steps.\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Accuracy, Precision, Recall, and F1-Score.\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    \n",
    "    total = len(y_true)\n",
    "    \n",
    "    accuracy = (tp + tn) / total if total > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1_score,\n",
    "        'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn\n",
    "    }\n",
    "\n",
    "# --- 2. Execution of Classification Experiment (Part b) ---\n",
    "# Re-running the necessary setup steps to ensure data integrity.\n",
    "\n",
    "# Define the True Structure and Naive Bayes Structure\n",
    "COLUMNS = ['asia', 'tub', 'smoke', 'bronc', 'either', 'xray', 'dysp', 'lung']\n",
    "TRUE_STRUCTURE_EDGES = {\n",
    "    'asia': [], 'smoke': [], \n",
    "    'tub': ['asia'], \n",
    "    'lung': ['smoke'], \n",
    "    'bronc': ['smoke'],\n",
    "    'either': ['tub', 'lung'],\n",
    "    'xray': ['either'], \n",
    "    'dysp': ['either', 'bronc']\n",
    "}\n",
    "\n",
    "# Naive Bayes Structure \n",
    "naive_structure = {'lung': []}\n",
    "for node in COLUMNS:\n",
    "    if node != 'lung':\n",
    "        naive_structure[node] = ['lung'] \n",
    "\n",
    "# 1. Select Optimal K2 Structure (Order 3, Max P=2)\n",
    "ORDER_3 = ['smoke', 'asia', 'lung', 'tub', 'bronc', 'either', 'dysp', 'xray']\n",
    "# This call assumes k2_algorithm, df_train, and all necessary data are available\n",
    "best_bn_k2 = k2_algorithm(df_train, ORDER_3, max_parents=2) \n",
    "\n",
    "# 2. Define the other two models (BN_True and BN_Naive)\n",
    "bn_true = BayesianNetwork(COLUMNS)\n",
    "bn_true.structure = TRUE_STRUCTURE_EDGES\n",
    "\n",
    "bn_naive = BayesianNetwork(COLUMNS)\n",
    "bn_naive.structure = naive_structure\n",
    "\n",
    "# 3. Combine Train and Validation data for training \n",
    "df_train_val = pd.concat([df_train, df_val]).reset_index(drop=True)\n",
    "\n",
    "# 4. Train CPTs on df_train_val\n",
    "best_bn_k2.learn_cpts(df_train_val)\n",
    "bn_true.learn_cpts(df_train_val)\n",
    "bn_naive.learn_cpts(df_train_val)\n",
    "\n",
    "# 5. Evaluate on Test data (df_test)\n",
    "y_true = df_test['lung'].values\n",
    "\n",
    "models = {\n",
    "    \"K2 Optimal (Max P=2)\": best_bn_k2,\n",
    "    \"True Structure (ASIA)\": bn_true,\n",
    "    \"Naive Bayes (Lung Target)\": bn_naive\n",
    "}\n",
    "\n",
    "classification_results = {}\n",
    "print(\"\\n--- Running Classification on Test Data (Target: Lung) ---\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    # This call assumes predict_classification is available and works as intended\n",
    "    y_pred = predict_classification(model, df_test, target_node='lung')\n",
    "    metrics = calculate_metrics(y_true, y_pred) \n",
    "    \n",
    "    classification_results[name] = metrics\n",
    "    \n",
    "    print(f\"{name}: Acc={metrics['Accuracy']:.4f}, F1={metrics['F1-Score']:.4f}\")\n",
    "\n",
    "# 6. Display Results Table (Text-based output to avoid ImportError)\n",
    "\n",
    "print(\"\\n--- Classification Results Summary (Test Data) ---\")\n",
    "print(f\"{'Model':<25} | {'Accuracy':<10} | {'Precision':<10} | {'Recall':<10} | {'F1-Score':<10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for name, metrics in classification_results.items():\n",
    "    print(\n",
    "        f\"{name:<25} | {metrics['Accuracy']:<10.4f} | {metrics['Precision']:<10.4f} | {metrics['Recall']:<10.4f} | {metrics['F1-Score']:<10.4f}\"\n",
    "    )\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e2a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Classification on Test Data (Target: Lung) ---\n",
      "K2 Optimal (Max P=2): Acc=0.9933, F1=0.8837\n",
      "True Structure (ASIA): Acc=0.9933, F1=0.8837\n",
      "Naive Bayes (Lung Target): Acc=0.9693, F1=0.6102\n",
      "\n",
      "--- Classification Results Summary (Test Data) ---\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\venv\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\importlib\\__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1206\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1178\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1142\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tabulate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 190\u001b[39m\n\u001b[32m    188\u001b[39m df_class_results = pd.DataFrame(classification_results).T\n\u001b[32m    189\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Classification Results Summary (Test Data) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_class_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# The image tag for the plot goes here\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:2994\u001b[39m, in \u001b[36mDataFrame.to_markdown\u001b[39m\u001b[34m(self, buf, mode, index, storage_options, **kwargs)\u001b[39m\n\u001b[32m   2992\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mtablefmt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2993\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mshowindex\u001b[39m\u001b[33m\"\u001b[39m, index)\n\u001b[32m-> \u001b[39m\u001b[32m2994\u001b[39m tabulate = \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtabulate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2995\u001b[39m result = tabulate.tabulate(\u001b[38;5;28mself\u001b[39m, **kwargs)\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\venv\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
     ]
    }
   ],
   "source": [
    "# Assuming the previous classes (BayesianNetwork, calculate_metrics) are defined\n",
    "\n",
    "# --- 5. True and Naive Bayesian Network Structures ---\n",
    "\n",
    "# The known structure of the ASIA network (for comparison)\n",
    "# A: asia, T: tub, S: smoke, L: lung, B: bronc, E: either, X: xray, D: dysp\n",
    "TRUE_STRUCTURE_EDGES = {\n",
    "    'asia': [], \n",
    "    'smoke': [], \n",
    "    'tub': ['asia'], \n",
    "    'lung': ['smoke'], \n",
    "    'bronc': ['smoke'],\n",
    "    'either': ['tub', 'lung'], # Key CPT with 2 parents\n",
    "    'xray': ['either'], \n",
    "    'dysp': ['either', 'bronc'] # Key CPT with 2 parents\n",
    "}\n",
    "\n",
    "# Naive Bayes Structure (Lung is the target node)\n",
    "# Assumption: All features are independent of each other given the class (Lung).\n",
    "# Note: For classification, features are parents of the class node, and features are marginally independent.\n",
    "# We will model the joint distribution P(L, F) = P(L) * P(F1|L) * P(F2|L) * ...\n",
    "NAIVE_BAYES_EDGES = {\n",
    "    'lung': [], # Lung is the root node\n",
    "    # All other nodes (features) are children of 'lung'\n",
    "    'asia': ['lung'], \n",
    "    'tub': ['lung'], \n",
    "    'smoke': ['lung'], \n",
    "    'bronc': ['lung'], \n",
    "    'either': ['lung'], \n",
    "    'xray': ['lung'], \n",
    "    'dysp': ['lung'] \n",
    "}\n",
    "\n",
    "# --- 6. Classification Prediction Logic ---\n",
    "\n",
    "def predict_classification(bn, data, target_node='lung'):\n",
    "    \"\"\"\n",
    "    Predicts the target variable using P(Target | Features) âˆ P(Target, Features)\n",
    "    P(Target, Features) is calculated by multiplying CPTs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify feature nodes (all nodes except the target)\n",
    "    feature_nodes = [n for n in bn.nodes if n != target_node]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for _, sample in data.iterrows():\n",
    "        # Calculate P(Target=0 | Features) and P(Target=1 | Features)\n",
    "        log_prob_0 = 0.0 # log(P(Target=0, Features))\n",
    "        log_prob_1 = 0.0 # log(P(Target=1, Features))\n",
    "        \n",
    "        target_states = [0, 1]\n",
    "        \n",
    "        for target_state in target_states:\n",
    "            current_log_prob = 0.0\n",
    "            \n",
    "            # Iterate through all nodes (Target and Features)\n",
    "            for node in bn.nodes:\n",
    "                cpts = bn.cpts[node]\n",
    "                parents = bn.structure[node]\n",
    "                \n",
    "                # Determine the value of the node and its parents for this calculation\n",
    "                \n",
    "                # If the node is the target, use the current target_state\n",
    "                node_value = target_state if node == target_node else sample[node]\n",
    "                \n",
    "                # If a parent is the target, use the current target_state\n",
    "                parent_config = tuple(target_state if p == target_node else sample[p] for p in parents)\n",
    "                \n",
    "                # Get the probability P(Node | Parents)\n",
    "                prob = 0.0\n",
    "                if parents:\n",
    "                    # Probabilities are stored in a Pandas Series or dict indexed by parent_config\n",
    "                    prob_series = cpts['P_1'] if node_value == 1 else cpts['P_0']\n",
    "                    \n",
    "                    if isinstance(prob_series, pd.Series):\n",
    "                        # Ensure we handle unseen parent configurations by adding a small value\n",
    "                        try:\n",
    "                            prob = prob_series.loc[parent_config]\n",
    "                        except KeyError:\n",
    "                            # If a parent config was not seen in training, use a small default (Laplace=1 should prevent this)\n",
    "                            prob = 1e-6 \n",
    "                    else: # If cpts are simple dictionaries (priors only, not series)\n",
    "                         prob = prob_series\n",
    "                else:\n",
    "                    # Node has no parents (prior probability)\n",
    "                    prob = cpts['P_1'] if node_value == 1 else cpts['P_0']\n",
    "                \n",
    "                current_log_prob += math.log(prob)\n",
    "            \n",
    "            if target_state == 0:\n",
    "                log_prob_0 = current_log_prob\n",
    "            else:\n",
    "                log_prob_1 = current_log_prob\n",
    "                \n",
    "        # Decision: P(T=1|F) > P(T=0|F)\n",
    "        # Since log(P(T=1, F)) vs log(P(T=0, F)) is equivalent, we compare log_prob\n",
    "        prediction = 1 if log_prob_1 > log_prob_0 else 0\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "    return np.array(predictions)\n",
    "\n",
    "# --- 7. Execution of Classification Experiment (Part b) ---\n",
    "\n",
    "# 1. Select Optimal K2 Structure (Order 3, Max P=2)\n",
    "# We need to re-run K2 algorithm to get the actual model object, since we only saved the score.\n",
    "# This assumes the k2_algorithm defined previously is available.\n",
    "# Since the actual object was not saved, we redefine the optimal structure found in the logs:\n",
    "# Order 3: ['smoke', 'asia', 'lung', 'tub', 'bronc', 'either', 'dysp', 'xray']\n",
    "# Max P: 2\n",
    "ORDER_3 = ['smoke', 'asia', 'lung', 'tub', 'bronc', 'either', 'dysp', 'xray']\n",
    "best_bn_k2 = k2_algorithm(df_train, ORDER_3, max_parents=2) # Retrain on df_train\n",
    "\n",
    "# 2. Define the other two models (BN_True and BN_Naive)\n",
    "bn_true = BayesianNetwork(COLUMNS)\n",
    "bn_true.structure = TRUE_STRUCTURE_EDGES\n",
    "\n",
    "# For Naive Bayes, we must define the node ordering for the network\n",
    "bn_naive_nodes = ['lung', 'asia', 'tub', 'smoke', 'bronc', 'either', 'xray', 'dysp']\n",
    "bn_naive = BayesianNetwork(bn_naive_nodes)\n",
    "bn_naive.structure = {n: NAIVE_BAYES_EDGES.get(n, []) for n in bn_naive_nodes}\n",
    "# The Naive Bayes structure as defined in the dictionary (Lung is independent, features depend on Lung)\n",
    "# We must ensure the actual nodes in the BN match the columns and the structure dictionary\n",
    "\n",
    "# Fix Naive Bayes structure:\n",
    "bn_naive = BayesianNetwork(COLUMNS)\n",
    "# Naive Bayes assumes features are independent *given the class*.\n",
    "# We enforce this by defining features as children of the target ('lung'), \n",
    "# and setting their structures based on the target.\n",
    "naive_structure = {'lung': []}\n",
    "for node in COLUMNS:\n",
    "    if node != 'lung':\n",
    "        # Feature node depends *only* on the target node\n",
    "        naive_structure[node] = ['lung'] \n",
    "bn_naive.structure = naive_structure\n",
    "bn_naive.nodes = COLUMNS\n",
    "\n",
    "\n",
    "# 3. Combine Train and Validation data for training (as requested)\n",
    "df_train_val = pd.concat([df_train, df_val]).reset_index(drop=True)\n",
    "\n",
    "# 4. Train CPTs on df_train_val\n",
    "best_bn_k2.learn_cpts(df_train_val)\n",
    "bn_true.learn_cpts(df_train_val)\n",
    "bn_naive.learn_cpts(df_train_val)\n",
    "\n",
    "# 5. Evaluate on Test data (df_test)\n",
    "y_true = df_test['lung'].values\n",
    "\n",
    "models = {\n",
    "    \"K2 Optimal (Max P=2)\": best_bn_k2,\n",
    "    \"True Structure (ASIA)\": bn_true,\n",
    "    \"Naive Bayes (Lung Target)\": bn_naive\n",
    "}\n",
    "\n",
    "classification_results = {}\n",
    "print(\"\\n--- Running Classification on Test Data (Target: Lung) ---\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = predict_classification(model, df_test, target_node='lung')\n",
    "    metrics = calculate_metrics(y_true, y_pred) # Reusing the F1 function from Q3/part 1\n",
    "    \n",
    "    # Add Accuracy, Precision, Recall calculation to calculate_metrics if needed, \n",
    "    # but for now, we extend the calculation here:\n",
    "    \n",
    "    # Recalculate full metrics here for completeness (since calculate_metrics was F1-only before)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    \n",
    "    accuracy = (tp + tn) / len(y_true) if len(y_true) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    classification_results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1_score\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}: Acc={accuracy:.4f}, F1={f1_score:.4f}\")\n",
    "\n",
    "# 6. Display Results Table\n",
    "\n",
    "df_class_results = pd.DataFrame(classification_results).T\n",
    "print(\"\\n--- Classification Results Summary (Test Data) ---\")\n",
    "print(df_class_results.to_markdown()) \n",
    "\n",
    "# The image tag for the plot goes here\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

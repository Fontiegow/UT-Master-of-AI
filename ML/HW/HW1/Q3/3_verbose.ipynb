{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af551af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustimNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_priors = {}\n",
    "        self.word_likelihoods = {}\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def fit(self, X_vec, y_labels, feature_names):\n",
    "        self.vocab_size = X_vec.shape[1]\n",
    "        self.feature_names = feature_names\n",
    "        total_samples = len(y_labels)\n",
    "\n",
    "        unique_classes = np.unique(y_labels)\n",
    "\n",
    "        self.vocab_size = X_vec.shape[1]\n",
    "        for c in unique_classes:\n",
    "            X_c = X_vec[y_labels == c]\n",
    "            total_tokens_in_c = np.sum(X_c)\n",
    "\n",
    "            denominator = total_tokens_in_c + self.alpha * self.vocab_size\n",
    "            self.word_likelihoods[c] = {}\n",
    "\n",
    "            word_counts = np.sum(X_c, axis=0)\n",
    "            for i, word in enumerate(self.feature_names):\n",
    "                numerator = word_counts[i] + self.alpha\n",
    "                self.word_likelihoods[c][word] = numerator / denominator\n",
    "\n",
    "    def predict_one(self, sample_vector):\n",
    "        best_class = None\n",
    "        max_log_prob = -np.inf\n",
    "\n",
    "        for c, prior in self.class_priors.items():\n",
    "            current_log_prob = np.log(prior)\n",
    "            for i, count in enumerate(sample_vector):\n",
    "                # We only need to consider words present in the sample (count > 0)\n",
    "                if count > 0:\n",
    "                    word = self.feature_names[i]\n",
    "                    \n",
    "                    # Get the likelihood P(W|C) for this word and class\n",
    "                    likelihood = self.word_likelihoods[c].get(word, 0)\n",
    "                    \n",
    "                    # Add the log likelihood for each occurrence of the word\n",
    "                    # We multiply the log(P(W|C)) by the count of the word, \n",
    "                    # based on the Multinomial Naive Bayes assumption.\n",
    "                    if likelihood > 0:\n",
    "                        current_log_prob += count * math.log(likelihood)\n",
    "            \n",
    "            # Check if this class is the best one so far\n",
    "            if current_log_prob > max_log_prob:\n",
    "                max_log_prob = current_log_prob\n",
    "                best_class = c\n",
    "                \n",
    "        return best_class\n",
    "\n",
    "    def predict(self, X_vec):\n",
    "        \"\"\"Predicts classes for an entire dataset.\"\"\"\n",
    "        predictions = [self.predict_one(sample) for sample in X_vec]\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254dada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# --- Helper Functions and Classes ---\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans raw text.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<br\\s*/>', ' ', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "STOP_WORDS = set([\n",
    "    'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'this', 'that', \n",
    "    'was', 'as', 'for', 'with', 'movie', 'film', 'but', 'on', 'are', \n",
    "    'not', 'have', 'be', 'one', 'all', 'at', 'by', 'an', 'who', 'so', \n",
    "    'from', 'like', 'there', 'or', 'just', 'about', 'out', 'if', 'has',\n",
    "    'what', 'some', 'good', 'can', 'more', 'when', 'very', 'up', 'no', \n",
    "    'time', 'my', 'even', 'would', 'she', 'which', 'only', 'really', \n",
    "    'see', 'story', 'their', 'had'\n",
    "])\n",
    "\n",
    "def tokenize_with_stopwords(text):\n",
    "    tokens = clean_text(text).split(\" \")\n",
    "    return [token for token in tokens if token and token not in STOP_WORDS]\n",
    "\n",
    "def tokenize_without_stopwords(text):\n",
    "    cleaned_tokens = clean_text(text).split(\" \")\n",
    "    return [token for token in cleaned_tokens if token not in STOP_WORDS and len(token) > 1]\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
